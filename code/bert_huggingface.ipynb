{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qY1lXPCAJg2E"
      },
      "source": [
        "# BERT tutorial using Hugging Face\n",
        "## æ•™å­¸ç›®æ¨™\n",
        "åˆ©ç”¨ Hugging Face å¥—ä»¶å¿«é€Ÿä½¿ç”¨ BERT æ¨¡å‹ä¾†é€²è¡Œä¸‹æ¸¸ä»»å‹™è¨“ç·´\n",
        "- å–®ä¸€å¥å‹åˆ†é¡ä»»å‹™ (single-sentence text classification)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNg9uH2-Jg2H"
      },
      "source": [
        "## é©ç”¨å°è±¡\n",
        "å·²ç¶“æœ‰åŸºæœ¬çš„æ©Ÿå™¨å­¸ç¿’çŸ¥è­˜ï¼Œä¸”æ“æœ‰ Pythonã€`numpy`ã€`pandas`ã€`scikit-learn` ä»¥åŠ `PyTorch` åŸºç¤çš„å­¸ç”Ÿã€‚\n",
        "\n",
        "è‹¥æ²’æœ‰å…ˆå­¸é Pythonï¼Œè«‹åƒè€ƒ [python-å…¥é–€èªæ³•](https://github.com/IKMLab/course_material/blob/master/python-å…¥é–€èªæ³•.ipynb) æ•™å­¸ã€‚\n",
        "\n",
        "è‹¥æ²’æœ‰å…ˆå­¸é `pandas`ï¼Œè«‹åƒè€ƒ [pandas-åŸºæœ¬åŠŸèƒ½](https://github.com/IKMLab/course_material/blob/master/pandas-åŸºæœ¬åŠŸèƒ½.ipynb) æ•™å­¸ã€‚\n",
        "\n",
        "è‹¥æ²’æœ‰å…ˆå­¸é `numpy`ï¼Œè«‹åƒè€ƒ [numpy-åŸºæœ¬åŠŸèƒ½](https://github.com/IKMLab/course_material/blob/master/numpy-åŸºæœ¬åŠŸèƒ½.ipynb) æ•™å­¸ã€‚\n",
        "\n",
        "è‹¥æ²’æœ‰å…ˆå­¸é `scikit-learn`ï¼Œè«‹åƒè€ƒ [scikit-learn-åŸºæœ¬åŠŸèƒ½](https://github.com/IKMLab/course_material/blob/master/scikit-learn-åŸºæœ¬åŠŸèƒ½.ipynb) æ•™å­¸ã€‚\n",
        "\n",
        "è‹¥æ²’æœ‰å…ˆå­¸é  `PyTorch` ï¼Œè«‹åƒè€ƒ [PyTorch-åŸºæœ¬åŠŸèƒ½](https://github.com/IKMLab/course_material/blob/master/PyTorch-åŸºæœ¬åŠŸèƒ½.ipynb) æ•™å­¸ã€‚\n",
        "\n",
        "è‹¥æ²’æœ‰å…ˆå­¸éå¦‚ä½•ä½¿ç”¨ `PyTorch` å»ºç«‹è‡ªç„¶èªè¨€è™•ç†åºåˆ—æ¨¡å‹ï¼Œè«‹åƒè€ƒ [NN-ä¸­æ–‡æ–‡æœ¬åˆ†é¡](https://github.com/IKMLab/course_material/blob/master/NN-ä¸­æ–‡æ–‡æœ¬åˆ†é¡.ipynb) æ•™å­¸ã€‚"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYSdGFduJg2H"
      },
      "source": [
        "## BERT ç°¡æ˜“ä»‹ç´¹\n",
        "### Word embeddings çš„å•é¡Œ\n",
        "![Imgur](https://i.imgur.com/h6U5k41.png)\n",
        "- æ¯å€‹å–®è©çš„æ„æ€åœ¨ä¸åŒçš„å ´åˆä¸‹æ‡‰è©²æœ‰ä¸åŒçš„æ„ç¾©è¡¨é”\n",
        "- æˆ‘å€‘å¯ä»¥åˆ©ç”¨ RNN ä½œç‚ºèªè¨€æ¨¡å‹ï¼Œé€éèªè¨€æ¨¡å‹çš„è¼¸å…¥èˆ‡è¼¸å‡ºçš„è™•ç†ä¾†ç”¢ç”Ÿèƒ½å¤ ç†è§£ä¸Šä¸‹æ–‡èªæ„çš„ contextual embeddings\n",
        "    - Language model: èªè¨€æ¨¡å‹ï¼Œè—‰ç”±ä¼°è¨ˆ(æˆ–æœ€ä½³åŒ–)ä¸€æ•´å€‹åºåˆ—çš„ç”Ÿæˆæ©Ÿç‡ä¾†è¼¸å‡ºå­—è©çš„æ¨¡å‹\n",
        "        - å¯ä»¥åƒè€ƒ [language model çš„è©³ç´°æ•™å­¸](https://youtu.be/LheoxKjeop8?t=50)\n",
        "- è—‰ç”±æ­¤ç¨®åšæ³•ï¼Œæˆ‘å€‘å¯ä»¥å°‡å–®è©èªæ„çš„ word embeddings è½‰æ›ç‚ºå…·æœ‰ä¸Šä¸‹æ–‡èªæ„çš„ contextual embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7n0nQj0qJg2I"
      },
      "source": [
        "## æ‰€ä»¥ä»€éº¼æ˜¯ BERT?\n",
        "- è«‹åƒè€ƒç†è«–å±¤é¢çš„è©³ç´°æ•™å­¸ ([å½±ç‰‡é€£çµ](https://www.youtube.com/watch?v=gh0hewYkjgo))\n",
        "- æƒ³é€²è¡Œ PyTorch çš„ BERT å¯¦ä½œä¾†ç²å¾—æ·±å…¥ç†è§£å¯ä»¥åƒè€ƒ ([ç¶²èªŒé€£çµ](https://leemeng.tw/attack_on_bert_transfer_learning_in_nlp.html))\n",
        "- ä¹Ÿå¯ä»¥åƒè€ƒ Jay Alammar çš„ The Illustrated BERT ([ç¶²èªŒé€£çµ](https://jalammar.github.io/illustrated-bert/))\n",
        "- ä¹Ÿå¯ä»¥åƒè€ƒåŸå§‹è«–æ–‡ ([è«–æ–‡é€£çµ](https://www.aclweb.org/anthology/N19-1423/))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2Je-RIxJg2I"
      },
      "source": [
        "### BERT çš„ Pre-training å’Œ Fine-tuning èˆ‡å…ˆå‰æ–¹æ³•æ¯”è¼ƒ\n",
        "![Imgur](https://i.imgur.com/qfLhUaG.png)\n",
        "- Pre-training å·²ç¶“æ˜¯ NLP é ˜åŸŸä¸­ä¸å¯æˆ–ç¼ºçš„æ–¹æ³•\n",
        "- åƒ BERT é€™é¡åŸºæ–¼ Transformers çš„[æ¨¡å‹éå¸¸å¤š](http://speech.ee.ntu.edu.tw/~tlkagk/courses/DLHLP20/BERT%20train%20(v8).pdf)ï¼Œå¯ä»¥å‰å¾€ [Hugging Face models](https://huggingface.co/models) ä¸€è¦½ç©¶ç«Ÿ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OT5oltZrJg2J"
      },
      "source": [
        "## Hugging Face ä»‹ç´¹\n",
        "- ğŸ¤— Hugging Face æ˜¯å°ˆé–€æä¾›è‡ªç„¶èªè¨€è™•ç†é ˜åŸŸçš„å‡½å¼åº«\n",
        "- å…¶å‡½å¼åº«æ”¯æ´ PyTorch å’Œ TensorFlow\n",
        "- ğŸ¤— Hugging Face çš„ä¸»è¦å¥—ä»¶ç‚º:\n",
        "    1. Transformers ([é€£çµ](https://huggingface.co/transformers/index.html))\n",
        "    - æä¾›äº†ç¾ä»Šæœ€å¼·å¤§çš„è‡ªç„¶èªè¨€è™•ç†æ¨¡å‹ï¼Œä½¿ç”¨ä¸Šéå¸¸å½ˆæ€§ä¸”æ–¹ä¾¿\n",
        "    2. Tokenizers ([é€£çµ](https://huggingface.co/docs/tokenizers/python/latest/))\n",
        "    - è®“ä½ å¯ä»¥å¿«é€Ÿåšå¥½ BERT ç³»åˆ—æ¨¡å‹ tokenization\n",
        "    3. Datasets ([é€£çµ](https://huggingface.co/docs/datasets/))\n",
        "    - æä¾›å¤šç¨®è‡ªç„¶èªè¨€è™•ç†ä»»å‹™çš„è³‡æ–™é›†"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TUjyDQ-qJg2J"
      },
      "outputs": [],
      "source": [
        "# !pip install torch==2.4.0\n",
        "# !pip install transformers==4.37.0\n",
        "!pip install datasets\n",
        "# !pip install accelerate==0.21.0\n",
        "# !pip install scikit-learn==1.5.2\n",
        "!pip install wget\n",
        "# !pip install tarfile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z3tiYaLoJg2K"
      },
      "outputs": [],
      "source": [
        "# 1. Check the versions of your packages\n",
        "import torch\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "\n",
        "import transformers\n",
        "print(f\"Hugging Face Transformers version: {transformers.__version__}\")\n",
        "\n",
        "import datasets\n",
        "print(f\"Hugging Face Datasets version: {datasets.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s-JMEX4oJg2L"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "from pathlib import Path # (Python3.4+)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpZYuEYpJg2M"
      },
      "source": [
        "# å–®ä¸€å¥å‹åˆ†é¡ä»»å‹™ (single-sentence text classification)\n",
        "## æº–å‚™è³‡æ–™é›† (éœ€å…ˆä¸‹è¼‰)\n",
        "æˆ‘å€‘ä½¿ç”¨ IMDb reviews è³‡æ–™é›†ä½œç‚ºç¯„ä¾‹"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-NyUXdw-Jg2M"
      },
      "outputs": [],
      "source": [
        "# ä¸‹è¼‰ IMDb è³‡æ–™é›†\n",
        "import wget\n",
        "url = 'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
        "filename = wget.download(url, out='./')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Be-MEg3mJg2M"
      },
      "outputs": [],
      "source": [
        "# è§£å£“ç¸® IMDb è³‡æ–™é›†\n",
        "\n",
        "import tarfile\n",
        "\n",
        "# æŒ‡å®šæª”æ¡ˆä½ç½®ï¼Œä¸¦è§£å£“ç¸® .gz çµå°¾çš„å£“ç¸®æª”\n",
        "tar = tarfile.open('aclImdb_v1.tar.gz', 'r:gz')\n",
        "tar.extractall()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "je_x5Wa-Jg2N"
      },
      "source": [
        "## æ¥ä¸‹ä¾†æˆ‘å€‘è¦é€²è¡Œè³‡æ–™å‰è™•ç†\n",
        "ä½†é¦–å…ˆè¦è§€å¯Ÿè§£å£“ç¸®å¾Œçš„è³‡æ–™å¤¾çµæ§‹:\n",
        "```\n",
        "aclImdb---\n",
        "        |--train\n",
        "        |    |--neg\n",
        "        |    |--pos\n",
        "        |    |--...\n",
        "        |--test\n",
        "        |    |--neg\n",
        "        |    |--pos\n",
        "        |    |--...\n",
        "        |--imdb.vocab\n",
        "        |--imdbEr.text\n",
        "        |--README\n",
        "```\n",
        "å…¶ä¸­ train å’Œ test è³‡æ–™å¤¾ä¸­åˆ†åˆ¥åˆæœ‰ neg å’Œ pos å…©ç¨®è³‡æ–™å¤¾\n",
        "\n",
        "æˆ‘å€‘è¦é‡å°é€™å…©å€‹ç›®æ¨™è³‡æ–™å¤¾é€²è¡Œè™•ç†"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nPGum-cuJg2N"
      },
      "outputs": [],
      "source": [
        "# Create a function to pre-process the IMDb dataset\n",
        "def read_imdb_split(split_dir):\n",
        "    split_dir = Path(split_dir)\n",
        "    texts, labels = [], []\n",
        "    for label_dir in [\"pos\", \"neg\"]:\n",
        "        # Use glob() to get files with the extension \".txt\"\n",
        "        for text_file in (split_dir/label_dir).glob(\"*.txt\"):\n",
        "            # read_text() returns the decoded contents of the pointed-to file as a string\n",
        "            tmp_text = text_file.read_text()\n",
        "\n",
        "            # Append the read text to the list we defined in advance\n",
        "            texts.append(tmp_text)\n",
        "\n",
        "            # Build labels based on the folder name\n",
        "            labels.append(0 if label_dir == \"neg\" else 1)\n",
        "\n",
        "    return texts, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yaFWBwWTJg2N"
      },
      "outputs": [],
      "source": [
        "# Pre-process the IMDb dataset (execution)\n",
        "\n",
        "train_texts, train_labels = read_imdb_split('aclImdb/train')\n",
        "test_texts, test_labels = read_imdb_split('aclImdb/test')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LR_fGzKyJg2N"
      },
      "source": [
        "### åˆ‡åˆ†è¨“ç·´è³‡æ–™ï¼Œä¾†åˆ†å‡º validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ikvd4SfUJg2N"
      },
      "outputs": [],
      "source": [
        "# Use train_test_split to split the training data into training and validation data\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Set a random seed for reproducibility\n",
        "random_seed = 42\n",
        "\n",
        "# Set the ratio of the validation set to the training set\n",
        "valid_ratio = 0.2\n",
        "\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    train_texts,\n",
        "    train_labels,\n",
        "    test_size=valid_ratio,\n",
        "    random_state=random_seed\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vl-ti6RvJg2N"
      },
      "source": [
        "## è¼¸å…¥ BERT çš„å‰è™•ç†\n",
        "![Imgur](https://i.imgur.com/3C7xDlf.png)\n",
        "(åœ–ç‰‡ä¾†æº: BERT [åŸå§‹è«–æ–‡](https://www.aclweb.org/anthology/N19-1423/))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "084CRnJqJg2N"
      },
      "source": [
        "### Tokenization\n",
        "- æ–·å­—çš„éƒ¨ä»½ä»¥ DistilBERT (Sanh et al., 2019) çš„ tokenizer ç‚ºä¾‹\n",
        "- Hugging Face çš„ tokenizer å¯ä»¥ç›´æ¥å¹«ä½ è‡ªå‹•å°‡è³‡æ–™è½‰æ›æˆ BERT çš„è¼¸å…¥å‹å¼ (ä¹Ÿå°±æ˜¯åŠ å…¥[CLS]å’Œ[SEP] tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dW2UFRcGJg2N"
      },
      "source": [
        "## Hugging Face AutoTokenizer\n",
        "- ä½¿ç”¨ AutoTokenizer æ­é… Hugging Face models çš„åç¨±å¯ä»¥ç›´æ¥å‘¼å«ä½¿ç”¨\n",
        "- èˆ‰ä¾‹:\n",
        "    - transformers.AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "    - ç­‰åŒæ–¼ transformers.DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "- [é»é€™è£¡ä¾†æŸ¥çœ‹ Hugging Face models çš„åç¨±](https://huggingface.co/transformers/pretrained_models.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "smRMfe1RJg2N"
      },
      "outputs": [],
      "source": [
        "# Load the Hugging Face tokenizer\n",
        "\n",
        "model_name = \"bert-base-uncased\"\n",
        "# Use .from_pretrained() for a pre-trained model\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9wB1drpFJg2N"
      },
      "outputs": [],
      "source": [
        "# Perform tokenization for the train / val / test datas\n",
        "# truncation: ä»£è¡¨ä¾ç…§ max_length é€²è¡Œåºåˆ—é•·åº¦çš„è£åˆ‡\n",
        "# max_length å¯ä»¥åœ¨ tokenizer çš„ parameters ä¸­é€²è¡Œè¨­å®š\n",
        "# å¦‚æœæ²’æœ‰æŒ‡å®š max_lengthï¼Œå‰‡ä¾ç…§æ‰€ä½¿ç”¨çš„æ¨¡å‹çš„åºåˆ—æœ€å¤§é•·åº¦\n",
        "# padding ç‚º True è¡¨ç¤ºæœƒå°‡åºåˆ—é•·åº¦è£œé½Šè‡³è©² batch çš„æœ€å¤§é•·åº¦ (æ¬²çŸ¥è©³æƒ…è«‹æŸ¥çœ‹ source code)\n",
        "\n",
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
        "val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n",
        "test_encodings = tokenizer(test_texts, truncation=True, padding=True)\n",
        "\n",
        "# ä½†é€™æ¨£çš„è©±æ‰€æœ‰çš„å¥å­éƒ½æœƒè¢« padded åˆ°ä¸€æ¨£çš„é•·åº¦"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VDGsIlStJg2O"
      },
      "outputs": [],
      "source": [
        "# æŸ¥çœ‹ max_length\n",
        "\n",
        "tokenizer.model_max_length"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# æŸ¥çœ‹ tokenizer çš„åŠŸèƒ½\n",
        "\n",
        "tokenizer_methods = dir(tokenizer)\n",
        "\n",
        "# Group them into rows for better readability\n",
        "for i in range(0, len(tokenizer_methods), 5):\n",
        "    print(\", \".join(tokenizer_methods[i:i+5]))\n",
        "    tokenizer_methods = dir(tokenizer)"
      ],
      "metadata": {
        "id": "Egy0r5Gkc4lN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76aDSEWVJg2O"
      },
      "outputs": [],
      "source": [
        "# æŸ¥çœ‹ [CLS] token å’Œ [SEP] token åœ¨å­—å…¸ä¸­çš„ ID\n",
        "\n",
        "print(\"The ID of [CLS] token is {}.\".format(tokenizer.vocab[\"[CLS]\"]))\n",
        "print(\"The ID of [SEP] token is {}.\".format(tokenizer.vocab[\"[SEP]\"]))\n",
        "print(\"The ID of [PAD] token is {}.\".format(tokenizer.vocab[\"[PAD]\"]))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# æŸ¥çœ‹ output encodings çš„ key å€¼å…§å®¹\n",
        "\n",
        "print(val_encodings.keys())"
      ],
      "metadata": {
        "id": "hEBy_LVNeAqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# æŸ¥çœ‹ output encodings æ˜¯ä»€éº¼å‹æ…‹\n",
        "\n",
        "print(type(val_encodings))"
      ],
      "metadata": {
        "id": "-o7LHsjheKpF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfGCHkFpJg2O"
      },
      "source": [
        "### æª¢æŸ¥ tokenization å¾Œçš„çµæœ\n",
        "- ä½¿ç”¨ Hugging Face tokenizer é€²è¡Œ tokenization å¾Œçš„çµæœæ˜¯ä¸€å€‹ dict\n",
        "- é€™å€‹ dict çš„ keys åŒ…å« 'input_ids' å’Œ 'attention_mask'\n",
        "- input_ids: åŸæœ¬å¥å­ä¸­çš„æ¯å€‹å­—è©è¢«æ–·è©å¾Œè½‰æ›æˆå­—å…¸çš„ ID\n",
        "    - æ³¨æ„!! tokenizer å°å°çš„å‹•ä½œå·²ç¶“å¹«ä½ å®Œæˆäº†æ–·è©å’Œ word to ID çš„è½‰æ›\n",
        "- attention_mask: tokenization å¾Œå¥å­ä¸­åŒ…å«æ–‡å­—çš„éƒ¨åˆ†ç‚º 1ï¼Œpadding çš„éƒ¨åˆ†ç‚º 0\n",
        "    - å¯ä»¥æƒ³åƒæˆæ¨¡å‹éœ€è¦æŠŠæ³¨æ„åŠ›æ”¾åœ¨æœ‰æ–‡å­—çš„ä½ç½®"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Bm0BoZIJg2O"
      },
      "outputs": [],
      "source": [
        "# æª¢æŸ¥ tokenization å¾Œçš„çµæœ\n",
        "\n",
        "print(val_encodings.input_ids[0])\n",
        "print(val_encodings.token_type_ids[0])\n",
        "print(val_encodings.attention_mask[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hlrnqw9xJg2O"
      },
      "outputs": [],
      "source": [
        "class IMDbDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Note that the tokenizer output is a dict wrapper\n",
        "        # Convert data and labels into PyTorch tensors\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        # Number of a dataset\n",
        "        return len(self.labels)\n",
        "\n",
        "train_dataset = IMDbDataset(train_encodings, train_labels)\n",
        "val_dataset = IMDbDataset(val_encodings, val_labels)\n",
        "test_dataset = IMDbDataset(test_encodings, test_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlHobatFJg2O"
      },
      "source": [
        "### é™¤äº†è‡ªå·±è™•ç†è³‡æ–™ï¼Œä½ é‚„å¯ä»¥ä½¿ç”¨ Hugging Face Datasets\n",
        "- Hugging Face Datasets å·²ç¶“å¹«ä½ æ”¶éŒ„äº†è‡ªç„¶èªè¨€è™•ç†é ˜åŸŸå¸¸è¦‹çš„è³‡æ–™é›†\n",
        "- ç›´æ¥å‘¼å« Datasets ä¸¦æ­é…ä¸‹é¢å¹¾å€‹ cells çš„èªæ³•ï¼Œå¯çœä¸‹ä¸å°‘æ™‚é–“\n",
        "- ä½†å‰ææ˜¯ä½ è¦é€²è¡Œçš„ä»»å‹™è³‡æ–™é›†æœ‰è¢«æ”¶éŒ„åœ¨ Hugging Face Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o5nX6CEYJg2O"
      },
      "outputs": [],
      "source": [
        "# Load the IMDb training set\n",
        "train = datasets.load_dataset(\"imdb\", split=\"train\")\n",
        "\n",
        "# Split the validation set\n",
        "random_seed = 42\n",
        "splits = train.train_test_split(\n",
        "    test_size=0.2,\n",
        "    seed=random_seed\n",
        ")\n",
        "train, valid = splits['train'], splits['test']\n",
        "\n",
        "# Load the IMDb test set\n",
        "test = datasets.load_dataset(\"imdb\", split=\"test\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mOT5JmqFJg2O"
      },
      "outputs": [],
      "source": [
        "def to_torch_data(hug_dataset):\n",
        "    \"\"\"Transform Hugging Face Datasets into PyTorch Dataset\n",
        "    Args:\n",
        "        - hug_dataset: data loaded from HF Datasets\n",
        "    Return:\n",
        "        - dataset: PyTorch Dataset\n",
        "    \"\"\"\n",
        "    dataset = hug_dataset.map(\n",
        "        lambda batch: tokenizer(\n",
        "            batch[\"text\"],\n",
        "            truncation=True,\n",
        "            padding=True\n",
        "        ),\n",
        "        batched=True\n",
        "    )\n",
        "    # dataset.set_format(\n",
        "    #     type='torch',\n",
        "    #     columns=[\n",
        "    #         'input_ids',\n",
        "    #         'attention_mask',\n",
        "    #         'label'\n",
        "    #     ]\n",
        "    # )\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4I1PF4EDaVOY"
      },
      "outputs": [],
      "source": [
        "train_dataset = to_torch_data(train)\n",
        "val_dataset = to_torch_data(valid)\n",
        "test_dataset = to_torch_data(test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_dataset.format['type'])"
      ],
      "metadata": {
        "id": "dfq-qR4jh6qZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MF4ZQrNJg2O"
      },
      "source": [
        "## ä½¿ç”¨ Hugging Face çš„æ¨¡å‹\n",
        "- åœ¨é€™å€‹ API ç››è¡Œçš„ä¸–ä»£ï¼Œç¸½æ˜¯æœ‰äººå¹«ä½ è¨­æƒ³å‘¨åˆ°\n",
        "- [Hugging Face çš„æ¨¡å‹é é¢é€£çµ](https://huggingface.co/models)\n",
        "- ä»¥ BERT ç‚ºä¾‹ï¼Œåªè¦é€é AutoModel.from_pretrained(\"bert-base-uncased\")ï¼Œå°±å¯ä»¥ç›´æ¥ä½¿ç”¨ BertModel\n",
        "- éœ€è¦æ³¨æ„çš„æ˜¯æ¥ä¸‹ä¾†ä½ è¦åšæ€æ¨£çš„ä¸‹æ¸¸ä»»å‹™è¨“ç·´\n",
        "- åŒæ¨£ä»¥ BERT ç‚ºä¾‹ï¼Œåœ¨åŸå§‹è«–æ–‡ä¸­ BERT é€²è¡Œéä»¥ä¸‹çš„ä»»å‹™:\n",
        "    - Sentence pair classification: MNLI/QQP/QNLI/MRPC/RTE/WNLI\n",
        "        - å°æ‡‰ `BertForSequenceClassification`\n",
        "        - ä½¿ç”¨é›™å¥çµåˆï¼Œä¸¦ä»¥åˆ†é¡çš„æ–¹å¼é€²è¡Œè¨“ç·´\n",
        "    - Semantic textual similarity: STS-B\n",
        "        - `BertForSequenceClassification`\n",
        "        - ä½¿ç”¨é›™å¥çµåˆï¼Œä¸¦ä»¥è¿´æ­¸çš„æ–¹å¼é€²è¡Œè¨“ç·´\n",
        "    - Single sentence classification: SST-2/CoLA\n",
        "        - å°æ‡‰ `BertForSequenceClassification`\n",
        "        - ä½¿ç”¨å–®å¥ï¼Œä¸¦ä»¥è¿´æ­¸çš„æ–¹å¼é€²è¡Œè¨“ç·´\n",
        "    - Question answering: SQuAD v1.1/v2.0\n",
        "        - å°æ‡‰ `BertForQuestionAnswering`\n",
        "        - ä½¿ç”¨é›™å¥(å•é¡Œ+åŸæ–‡)ï¼Œä¸¦é€éç­”æ¡ˆåœ¨åŸæ–‡ä¸­çš„ä½ç½®é€²è¡Œè¨“ç·´\n",
        "    - Named-entity recognition (slot filling): CoNLL-2003\n",
        "        - å°æ‡‰ `BertForTokenClassification`\n",
        "        - ä½¿ç”¨å–®å¥ï¼Œä¸¦ä»¥åˆ†é¡çš„æ–¹å¼é€²è¡Œè¨“ç·´\n",
        "- å¦‚æœè¦é€²è¡Œçš„ä¸‹æ¸¸ä»»å‹™è¨“ç·´ä¸åœ¨ Hugging Face å·²ç¶“å»ºå¥½çš„æ¨¡å‹ç¯„åœï¼Œé‚£å°±éœ€è¦è‡ªå·±å¯«ä¸€å€‹ model class:\n",
        "    1. ç¹¼æ‰¿ torch.nn.Module\n",
        "    2. åˆ©ç”¨ super ä¾†ç¹¼æ‰¿æ‰€æœ‰è¦ªå±¬é¡åˆ¥çš„å¯¦é«”å±¬æ€§\n",
        "    3. å®šç¾©æ¬²ä½¿ç”¨çš„ pre-trained model\n",
        "    4. å®šç¾©æœƒä½¿ç”¨åˆ°çš„å±¤å¦‚ linear æˆ– Dropout ç­‰\n",
        "    5. è¨­è¨ˆ forward function ä¸¦ä¸”è¨­å®šä¸‹æ¸¸ä»»å‹™çš„è¼¸å‡º"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XeyIxdSsJg2O"
      },
      "outputs": [],
      "source": [
        "# åˆ©ç”¨ AutoModel å‘¼å«æ¨¡å‹\n",
        "model = transformers.AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\",\n",
        "    num_labels=3,\n",
        "    # problem_type=\"single_label_classification\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.config.problem_type)"
      ],
      "metadata": {
        "id": "q4JXd3FAhZAq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sFWgVD0Jg2P"
      },
      "source": [
        "## é€²è¡Œæ¨¡å‹çš„è¨“ç·´\n",
        "### ä½¿ç”¨ Hugging Face Trainer ([Documentation](https://huggingface.co/transformers/main_classes/trainer.html))\n",
        "- Trainer æ˜¯ Hugging Face ä¸­é«˜åº¦å°è£çš„å¥—ä»¶ä¹‹ä¸€ï¼Œè² è²¬æ¨¡å‹è¨“ç·´æ™‚æœŸçš„\"æµç¨‹\"\n",
        "- éå»æˆ‘å€‘è‡ªè¡Œå¯«è¨“ç·´æµç¨‹çš„ç¨‹å¼ç¢¼å¯ä»¥äº¤çµ¦ Trainer\n",
        "- Trainer éœ€è¦æ­é…ä½¿ç”¨ [TrainingArguments](https://huggingface.co/transformers/main_classes/trainer.html#transformers.TrainingArguments)\n",
        "    - TrainingArguments æ˜¯ Trainer æ‰€éœ€è¦çš„å¼•æ•¸"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KZgSNytdJg2P"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = np.argmax(pred.predictions, axis=1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i5oaU6rHaBB7"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wHfKgmDjJg2U"
      },
      "outputs": [],
      "source": [
        "training_args = transformers.TrainingArguments(\n",
        "    output_dir='./results',            # è¼¸å‡ºçš„è³‡æ–™å¤¾\n",
        "    num_train_epochs=3,                # ç¸½å…±è¦è¨“ç·´å¤šå°‘ epochs\n",
        "    learning_rate=2e-5,                # learning rate\n",
        "    per_device_train_batch_size=16,    # training æ™‚çš„ batch sizeï¼Œif 2 GPUs: 32\n",
        "    per_device_eval_batch_size=64,     # test æ™‚çš„ batch sizeï¼Œif 2 GPUs: 128\n",
        "    gradient_accumulation_steps=2,     # æ¢¯åº¦ç´¯ç©æ¬¡æ•¸ (ä»£è¡¨å¤šå°‘å€‹ steps æ‰æœƒæ›´æ–°ä¸€æ¬¡æ¨¡å‹)\n",
        "    lr_scheduler_type='linear',        # Learning rate åœ¨ warmup_steps ä¸­ä¸Šå‡å¾Œä¸‹é™\n",
        "    warmup_steps=500,                  # Learning rate å¾é›¶ç·©æ…¢ä¸Šå‡çš„ steps\n",
        "    weight_decay=0.01,                 # hyperparameter for optimizer\n",
        "    evaluation_strategy='steps',       # time unit to perform evaluation\n",
        "    save_strategy='steps',             # time unit to save checkpoints\n",
        "    save_steps=500,                    # how often to save checkpoints\n",
        "    eval_steps=500,                    # how often to perform evaluation\n",
        "    load_best_model_at_end=True,       # if loading the best checkpoint at the end of training\n",
        "    metric_for_best_model='eval_loss', # how to judge the best model\n",
        "    report_to='tensorboard',           # if saving TensorBoard records\n",
        "    save_total_limit=10,               # maximum number of saved checkpoints\n",
        "    logging_dir='./logs',              # folder for logs\n",
        "    logging_steps=10,                  # how often to save logs\n",
        "    seed=random_seed                   # for reproducibility control\n",
        ")\n",
        "\n",
        "trainer = transformers.Trainer(\n",
        "    model=model,                         # ğŸ¤— model\n",
        "    args=training_args,                  # the `TrainingArguments` you set\n",
        "    train_dataset=train_dataset,         # the training dataset\n",
        "    eval_dataset=val_dataset,            # the evaluation dataset\n",
        "    compute_metrics=compute_metrics      # evaluation metric\n",
        ")\n",
        "\n",
        "# Use 1 GPU for training\n",
        "trainer.args._n_gpu=1\n",
        "\n",
        "# start training\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir ./logs"
      ],
      "metadata": {
        "id": "qlJK6jMskXg2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c_kNcENTJg2U"
      },
      "outputs": [],
      "source": [
        "# æ¸¬è©¦æ¨¡å‹\n",
        "\n",
        "trainer.predict(test_dataset)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}