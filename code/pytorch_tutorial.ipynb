{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# PyTorch 基礎教學\n",
        "## 教學目標\n",
        "\n",
        "這份教學的目標是介紹基礎 PyTorch 語法，幫助同學學習未來用來撰寫深度學習模型的函式庫。"
      ],
      "metadata": {
        "id": "ju3LGUXgIW7K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 簡介\n",
        "\n",
        "根據 [PyTorch 官方網站](https://pytorch.org/)（穩定版 v2.6.0）：\n",
        "\n",
        "> PyTorch is an open source machine learning framework that accelerates the path from research prototyping to production deployment.\n",
        ">\n",
        "> PyTorch 是一個開源的機器學習框架，能夠幫助加速從研究原型到商業應用的轉換過程。\n",
        "\n",
        "![PyTorch usage statistics](https://thegradient.pub/content/images/2019/10/ratio_medium-1.png)\n",
        "\n",
        "根據[統計](https://thegradient.pub/state-of-ml-frameworks-2019-pytorch-dominates-research-tensorflow-dominates-industry/)，PyTorch 在各大機器學習會議使用率逐年上升，使用者選擇 PyTorch 的原因為：\n",
        "\n",
        "- 簡潔，使用 `Python` 作為介面，且操作方法與 `NumPy` 相似\n",
        "- 好用的函式介面，沒有過多的抽象化\n",
        "- 執行效能佳"
      ],
      "metadata": {
        "id": "LfM23AFtWSID"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tU7SVKGyCRbh"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\n",
        "    # 確認 torch 的版本\n",
        "    f'PyTorch version {torch.__version__}\\n' +\n",
        "    # 確認是否有 GPU 裝置\n",
        "    f'GPU-enabled installation? {torch.cuda.is_available()}'\n",
        ")"
      ],
      "metadata": {
        "id": "LcYJSHOyC6VI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 張量宣告\n",
        "\n",
        "在 `torch` 中陣列稱為張量（Tensor），創造張量的語法為 `torch.tensor([value1, value2, ...])`。\n",
        "\n",
        "- 每個 `torch.Tensor` 都有不同的**數值型態屬性** `torch.Tensor.dtype`\n",
        "    - 必須透過 `torch.Tensor.dtype` 取得，無法透過 `type()` 取得\n",
        "- 可以指定型態\n",
        "    - 透過參數 `dtype` 指定型態\n",
        "    - 透過 `torch.LongTensor` 創造整數，預設為 `torch.int64`\n",
        "    - 透過 `torch.FloatTensor` 創造浮點數，預設為 `torch.float32`\n",
        "- [不同 Tensor 型態比較](https://pytorch.org/docs/stable/tensors.html)\n",
        "\n",
        "|`torch` 型態|`numpy` 型態|C 型態|範圍|\n",
        "|-|-|-|-|\n",
        "|`torch.int8`|`numpy.int8`|`int_8`|-128~127|\n",
        "|`torch.int16`|`numpy.int16`|`int_16`|-32768~32767|\n",
        "|`torch.int32`|`numpy.int32`|`int_32`|-2147483648~2147483647|\n",
        "|`torch.int64`|`numpy.int64`|`int_64`|-9223372036854775808~9223372036854775807|\n",
        "|`torch.float32`|`numpy.float32`|`float`||\n",
        "|`torch.float64`|`numpy.float64`|`double`||\n",
        "\n",
        "- 每個 `torch.Tensor` 都有**維度屬性** `torch.Size`\n",
        "    - 呼叫 `torch.Tensor.size()` 來取得維度屬性\n",
        "    - `torch.Tensor.size` 本質是 `tuple`\n",
        "    - 張量維度愈高，`len(torch.Tensor.size)` 數字愈大\n",
        "- 可以使用 `torch.Tensor.reshape` 或 `torch.Tensor.view` 進行維度變更\n",
        "    - 變更後的維度必須要與變更前的維度乘積相同\n",
        "    - 變更後的內容為 **shallow copy**"
      ],
      "metadata": {
        "id": "npVmIOnTCvYu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 張量宣告\n",
        "\n",
        "# 宣告 Tensor 變數\n",
        "t1 = torch.tensor([1, 2, 3])\n",
        "# 輸出 Tensor\n",
        "print(t1)\n",
        "# 輸出 True\n",
        "print(type(t1) == torch.Tensor)\n",
        "# 輸出 torch.int64\n",
        "print(t1.dtype)\n",
        "print()\n",
        "\n",
        "# 宣告 Tensor 變數\n",
        "t2 = torch.tensor([1., 2., 3.])\n",
        "# 輸出 Tensor\n",
        "print(t2)\n",
        "# 輸出 True\n",
        "print(type(t2) == torch.Tensor)\n",
        "# 輸出 torch.float32\n",
        "print(t2.dtype)\n",
        "print()\n",
        "\n",
        "# 各種 dtype\n",
        "# 輸出 torch.int8\n",
        "print(torch.tensor([1, 2], dtype=torch.int8).dtype)\n",
        "x = torch.tensor([1, 2], dtype=torch.int8)\n",
        "print(x)\n",
        "try:\n",
        "    x[0] = 128\n",
        "except Exception as e:\n",
        "    # int8 的範圍為 -128 ~ 127\n",
        "    # RuntimeError: value cannot be converted to type int8_t without overflow\n",
        "    print(e)\n",
        "print()"
      ],
      "metadata": {
        "id": "_M4nCFmKCwW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 宣告 LongTensor 變數 -> 通常 label 會使用這種型態，因為 label 通常是整數，在 torch 訓練的錯誤時可能會看到類似這種錯誤：\n",
        "# RuntimeError: Expected object of scalar type Long but got scalar type Float for argument #2 'target'\n",
        "# 此時就要記得檢查是否有使用到 LongTensor\n",
        "t3 = torch.LongTensor([1, 2, 3])\n",
        "# 輸出 torch.int64\n",
        "print(t3.dtype)\n",
        "\n",
        "# 宣告 FloatTensor 變數\n",
        "t4 = torch.FloatTensor([1, 2, 3])\n",
        "# 輸出 torch.float32\n",
        "print(t4.dtype)"
      ],
      "metadata": {
        "id": "C4wiQdVxDZoO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 隨機（Random）\n",
        "\n",
        "創造出新的張量，所有數值皆為**隨機決定**，必須**事先指定張量維度**。\n",
        "\n",
        "|函數|意義|用途|備註|\n",
        "|-|-|-|-|\n",
        "|`torch.empty`|創造隨機未初始化張量|已確認維度，尚未確認數值|無法控制隨機|\n",
        "|`torch.rand`|創造隨機浮點數張量，並符合均勻分佈|需要隨機浮點數時|透過均勻分佈決定亂數，範圍介於 0 到 1之間|\n",
        "|`torch.randn`|創造隨機浮點數張量，並符合常態分佈|需要符合常態分佈的隨機浮點數時|透過常態分佈決定亂數，$\\mu = 0$ 且 $\\sigma = 1$|\n",
        "|`torch.randint`|創造隨機整數張量|需要隨機整數時|透過均勻分佈決定亂數，可以控制隨機範圍|"
      ],
      "metadata": {
        "id": "wT8biNrtMMjo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 隨機\n",
        "\n",
        "# 隨機創造維度為 (2, 3) 的張量，數值為無法控制範圍的浮點\n",
        "print(torch.empty((2, 3)))\n",
        "print()\n",
        "\n",
        "# 隨機創造維度為 (2, 3) 的張量，數值為介於 0 到 1 之間的浮點\n",
        "print(torch.rand(2, 3))\n",
        "print()\n",
        "\n",
        "# 隨機創造維度為 (2, 3) 的張量，數值為介於 0 到 10 之間的浮點\n",
        "print(torch.rand(2, 3) * 10)\n",
        "print()\n",
        "\n",
        "# 隨機創造維度為 (2, 3) 的張量，數值為介於 -5 到 5 之間的浮點數\n",
        "print(torch.rand(2, 3) * 10 - 5)\n",
        "print()\n",
        "\n",
        "# 隨機創造維度為 (2, 3) 的張量，分佈為平均值為 0 標準差為 1 的常態分佈\n",
        "print(torch.randn(2, 3))\n",
        "print()\n",
        "\n",
        "# 隨機創造維度為 (2, 3) 的張量，數值為介於 -5 到 5 之間的浮點數\n",
        "print(torch.randint(-5, 5, size=(2, 3)))"
      ],
      "metadata": {
        "id": "NfQ5LMH_L9YB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f0f94ba-0176-4443-9d1a-b96fa9712c06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[7.0065e-45, 0.0000e+00, 7.0065e-45],\n",
            "        [7.0065e-45, 3.3631e-44, 0.0000e+00]])\n",
            "\n",
            "tensor([[0.6033, 0.3696, 0.9696],\n",
            "        [0.2471, 0.6108, 0.3590]])\n",
            "\n",
            "tensor([[3.9698, 3.7423, 6.3086],\n",
            "        [3.3357, 3.7079, 9.0378]])\n",
            "\n",
            "tensor([[-2.2183,  0.3285,  3.0421],\n",
            "        [ 4.7563,  2.8684,  2.6745]])\n",
            "\n",
            "tensor([[ 0.1936, -0.4561,  0.3376],\n",
            "        [ 1.1930,  1.7218, -1.6204]])\n",
            "\n",
            "tensor([[ 0,  4,  2],\n",
            "        [ 0, -5, -2]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 指定數值（Filled In）\n",
        "\n",
        "**快速創造**擁有特定數值的張量，必須**事先指定張量維度**。\n",
        "\n",
        "|函數|意義|用途|\n",
        "|-|-|-|\n",
        "|`torch.zeros`|創造指定維度大小的張量，所有數值初始化為 0|快速初始化|\n",
        "|`torch.zeros_like`|複製指定張量的維度，創造出新的張量，所有數值初始化為 0|複製張量並初始化|\n",
        "|`torch.ones`|創造指定維度大小的張量，所有數值初始化為 1|快速初始化|\n",
        "|`torch.ones_like`|複製指定張量的維度，創造出新的張量，所有數值初始化為 1|複製張量並初始化|\n",
        "|`torch.full`|創造指定維度大小的張量，所有數值初始化為指定數值|快速初始化|\n",
        "|`torch.full_like`|複製指定張量的維度，創造出新的張量，所有數值初始化為指定數值|複製張量並初始化|\n",
        "|`torch.eye`|創造單位矩陣|矩陣微分|\n",
        "|`torch.arange`|列舉數字|等同於 `list(range(value))`|"
      ],
      "metadata": {
        "id": "rxoa39d0NITU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 指定數值\n",
        "# 創造維度為 (2, 3) 的張量，並初始化為 0\n",
        "print(torch.zeros((2, 3)))\n",
        "print()\n",
        "\n",
        "# 宣告 Tensor 變數\n",
        "t5 = torch.tensor([\n",
        "    [1, 2, 3],\n",
        "    [4, 5, 6],\n",
        "])\n",
        "# 複製張量 t5 的維度，創造出新的張量，並初始化為 0\n",
        "print(torch.zeros_like(t5))\n",
        "print()\n",
        "\n",
        "# 創造維度為 (3, 4) 的張量，並初始化為 1\n",
        "print(torch.ones((3, 4)))\n",
        "print()\n",
        "\n",
        "# 宣告 Tensor 變數\n",
        "t6 = torch.tensor([\n",
        "    [1, 2, 3, 4],\n",
        "    [5, 6, 7, 8],\n",
        "    [9, 10, 11, 12]\n",
        "])\n",
        "# 複製張量 t6 的維度，創造出新的張量，並初始化為 1\n",
        "# like 的概念就是「模仿我丟給你的這個 tensor 的維度」\n",
        "print(torch.ones_like(t6))\n",
        "print()\n",
        "\n",
        "# 創造維度為 (5, 6) 的張量，並初始化為 420\n",
        "print(torch.full((5, 6), 420))\n",
        "print()\n",
        "\n",
        "# 宣告 Tensor 變數\n",
        "t7 = torch.tensor([\n",
        "    [1, 2, 3, 4, 5, 6],\n",
        "    [7, 8, 9, 10, 11, 12],\n",
        "    [13, 14, 15, 16, 17, 18],\n",
        "    [19, 20, 21, 22, 23, 24],\n",
        "    [25, 26, 27, 28, 29, 30]\n",
        "])\n",
        "# 複製張量 t7 的維度，創造出新的張量，並初始化為 69\n",
        "print(torch.full_like(t7, 69))"
      ],
      "metadata": {
        "id": "Zsw8WPOsNKcV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c8acad9-c9a7-4a26-8f3b-60feef09ba0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.]])\n",
            "\n",
            "tensor([[0, 0, 0],\n",
            "        [0, 0, 0]])\n",
            "\n",
            "tensor([[1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.]])\n",
            "\n",
            "tensor([[1, 1, 1, 1],\n",
            "        [1, 1, 1, 1],\n",
            "        [1, 1, 1, 1]])\n",
            "\n",
            "tensor([[420, 420, 420, 420, 420, 420],\n",
            "        [420, 420, 420, 420, 420, 420],\n",
            "        [420, 420, 420, 420, 420, 420],\n",
            "        [420, 420, 420, 420, 420, 420],\n",
            "        [420, 420, 420, 420, 420, 420]])\n",
            "\n",
            "tensor([[69, 69, 69, 69, 69, 69],\n",
            "        [69, 69, 69, 69, 69, 69],\n",
            "        [69, 69, 69, 69, 69, 69],\n",
            "        [69, 69, 69, 69, 69, 69],\n",
            "        [69, 69, 69, 69, 69, 69]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.zeros_like(t5))\n",
        "print(torch.ones_like(t5))\n",
        "print(torch.full_like(t5, 420))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lIQ4Sr3db8Yu",
        "outputId": "e41089fa-1f12-4158-ac85-22f4ff57bad9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0, 0, 0],\n",
            "        [0, 0, 0]])\n",
            "tensor([[1, 1, 1],\n",
            "        [1, 1, 1]])\n",
            "tensor([[420, 420, 420],\n",
            "        [420, 420, 420]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 創造 3x3 單位矩陣\n",
        "print(torch.eye(3))\n",
        "print()\n",
        "\n",
        "# 從 0 列舉至 10，但不包含 10\n",
        "print(torch.arange(10))\n",
        "print()\n",
        "\n",
        "# 從 6 列舉至 9，但不包含 9\n",
        "print(torch.arange(6, 9))\n",
        "print()\n",
        "\n",
        "# 從 4 遞增至 20，但不包含 20，每次遞增 7\n",
        "print(torch.arange(4, 20, 7))"
      ],
      "metadata": {
        "id": "0zjTX6H9NKxV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6fcd102-0286-4f52-cfd7-e3f279d92cb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 0., 0.],\n",
            "        [0., 1., 0.],\n",
            "        [0., 0., 1.]])\n",
            "\n",
            "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
            "\n",
            "tensor([6, 7, 8])\n",
            "\n",
            "tensor([ 4, 11, 18])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 從 numpy 轉換\n",
        "\n",
        "可以使用 `torch.tensor()` 將 `numpy.ndarray` 轉換成 `torch.Tensor`；\n",
        "使用 `torch.numpy()` 將 `torch.Tensor` 轉換成 `numpy.ndarray`。"
      ],
      "metadata": {
        "id": "CrnvfQubNK4w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 從 numpy 轉換\n",
        "\n",
        "# 宣告 ndarray 變數\n",
        "arr1 = np.array([1., 2., 3.])\n",
        "# 將 numpy.ndarray 轉換為 torch.Tensor\n",
        "t8 = torch.tensor(arr1)\n",
        "# 將 torch.Tensor 轉換為 numpy.ndarray\n",
        "arr2 = t8.numpy()\n",
        "\n",
        "print((\n",
        "    f'original numpy.ndarray: {arr1}, dtype: {arr1.dtype}\\n' +\n",
        "    f'converted torch.Tensor: {t8}, dtype: {t8.dtype}\\n' +\n",
        "    f'converted numpy.ndarray: {arr2}, dtype: {arr2.dtype}'\n",
        "))"
      ],
      "metadata": {
        "id": "scm_jQVCNTSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 張量取值\n",
        "\n",
        "與 `numpy` 語法概念相似。\n",
        "\n",
        "- 使用 `torch.Tensor[位置]` 來取得 `torch.Tensor` 中指定位置的值\n",
        "    - 若為**多個維度**的張量，則使用 `tuple` 來取得指定位置的值\n",
        "    - 若位置為**負數**，則等同於反向取得指定位置的值\n",
        "    - 取出的值會以 `torch.Tensor.dtype` 的形式保留\n",
        "- 使用 `torch.Tensor[起始位置:結束位置]` 來取得 `torch.Tensor` 中的部分**連續**值\n",
        "    - **包含起始位置**的值\n",
        "    - **不包含結束位置**的值\n",
        "    - 取出的值會以 `torch.Tensor` 的形式保留\n",
        "- 使用 `torch.Tensor[iterable]`（例如 `list`, `tuple` 等）來取得**多個** `torch.Tensor` 中的值\n",
        "    - 取出的值會以 `torch.Tensor` 的形式保留\n",
        "- 使用判斷式來取得 `torch.Tensor` 中的部份資料\n",
        "    - 經由判斷式所得結果也為 `torch.Tensor`\n",
        "    - 判斷式所得結果之 `torch.Tensor.dtype` 為**布林值** `bool`（`True` 或 `False`）\n",
        "    - 取出的值會以 `torch.Tensor` 的形式保留"
      ],
      "metadata": {
        "id": "5wnW64p7IL1y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 張量取值\n",
        "\n",
        "# 宣告 Tensor 變數\n",
        "t9 = torch.tensor([\n",
        "    [0, 1, 2],\n",
        "    [3, 4, 5],\n",
        "    [6, 7, 8],\n",
        "    [9, 10, 11],\n",
        "])\n",
        "\n",
        "# 輸出張量 t9 中的第 0 個位置的值 [0, 1, 2]\n",
        "print(t9[0])\n",
        "# 輸出張量 t9 中的第 1 個位置的值 [3, 4, 5]\n",
        "print(t9[1])\n",
        "# 輸出張量 t9 中的第 1 個位置的值 [6, 7, 8]\n",
        "# print(t9[2])\n",
        "# 輸出張量 t9 中的第 -2 個位置的值 [6, 7, 8]\n",
        "print(t9[-2])\n",
        "# 輸出張量 t9 中的第 -1 個位置的值 [9, 10, 11]\n",
        "print(t9[-1])\n",
        "print()"
      ],
      "metadata": {
        "id": "SZAMJ5oWH2Vk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4292f15-1b4e-4385-da9e-4636c9b46d7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0, 1, 2])\n",
            "tensor([3, 4, 5])\n",
            "tensor([6, 7, 8])\n",
            "tensor([ 9, 10, 11])\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 輸出張量 t9 中的第 [0, 1] 個位置的值 1\n",
        "print(t9[0, 1])\n",
        "# 輸出張量 t9 中的第 [1, 2] 個位置的值 5\n",
        "print(t9[1, 2])\n",
        "# 輸出張量 t9 中的第 [-1, -1] 個位置的值 11\n",
        "print(t9[-1, -1])\n",
        "# 輸出張量 t9 中的第 [-2, -1] 個位置的值 8\n",
        "print(t9[-2, -1])"
      ],
      "metadata": {
        "id": "3VzdyPIKe2jS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 取連續值\n",
        "\n",
        "# 宣告 Tensor 變數\n",
        "t10 = torch.tensor([\n",
        "    0, 10, 20, 30, 40,\n",
        "    50, 60, 70, 80, 90\n",
        "])\n",
        "\n",
        "# 輸出張量 t10 位置 0, 1, 2 但是不含位置 3 的值 [0, 10, 20]\n",
        "print(t10[0:3])\n",
        "# 輸出張量 t10 位置 7, 8, 9 的值 [70, 80, 90]\n",
        "print(t10[7:])\n",
        "# 輸出張量 t10 位置 0, 1 但是不含位置 2 的值 [0, 10]\n",
        "print(t10[:2])\n",
        "# 輸出張量 t10 所有位置的值 [0, 10, 20, 30, 40, 50, 60, 70, 80, 90]\n",
        "print(t10[:])\n",
        "print()\n",
        "\n",
        "# 宣告 Tensor 變數\n",
        "t11 = torch.tensor([\n",
        "    [0, 1, 2],\n",
        "    [3, 4, 5],\n",
        "    [6, 7, 8],\n",
        "    [9, 10, 11],\n",
        "])\n",
        "\n",
        "# 輸出張量 t11 位置 0, 1, 但是不含位置 2 的值 [[0, 1, 2], [3, 4, 5]]\n",
        "print(t11[0:2])\n",
        "# 輸出張量 t11 位置 1, 2, 3 的值 [[3, 4, 5], [6, 7, 8], [9, 10, 11]]\n",
        "print(t11[1:])\n",
        "# 輸出張量 t11 位置 0 但是不含位置 1 的值 [[0, 1, 2]]\n",
        "print(t11[:1])\n",
        "# 輸出張量 t11 位置 0 但是不含位置 1 的值 [[0, 1, 2], [3, 4, 5], [6, 7, 8], [9, 10, 11]]\n",
        "print(t11[:])"
      ],
      "metadata": {
        "id": "RII1mTrwIo4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 使用 iterable 取得多個值\n",
        "\n",
        "# 宣告 Tensor 變數\n",
        "t12 = torch.tensor([\n",
        "    0, 10, 20, 30, 40,\n",
        "    50, 60, 70, 80, 90\n",
        "])\n",
        "\n",
        "# 輸出張量 t12 中偶數位置的值 [0, 20, 40, 60, 80]\n",
        "print(t12[[0, 2, 4, 6, 8]])\n",
        "print()\n",
        "# 輸出張量 t12 中奇數位置的值 [10, 30, 50, 70, 90]\n",
        "print(t12[[1, 3, 5, 7, 9]])\n",
        "print()\n",
        "\n",
        "# 宣告 Tensor 變數\n",
        "t13 = torch.tensor([\n",
        "    [1, 2, 3, 4],\n",
        "    [5, 6, 7, 8],\n",
        "    [9, 10, 11, 12]\n",
        "])\n",
        "\n",
        "# 輸出張量 t13[0] 與 t13[1] 的值 [[1, 2, 3, 4] [5, 6, 7, 8]]\n",
        "print(t13[[0, 1]])\n",
        "# 輸出張量 t13[0, 2] 與 t13[1, 3] 的值 [3, 8]\n",
        "print(t13[[0, 1], [2, 3]])"
      ],
      "metadata": {
        "id": "JJY6ZLZUIrsx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 將張量 t13 位置 0 的所有數值改成 1995\n",
        "t13[0] = 1995\n",
        "print(t13)\n",
        "print()\n",
        "\n",
        "# 將張量 t13 位置 [0, 1] 的數值改成 10\n",
        "t13[0, 1] = 10\n",
        "print(t13)\n",
        "print()\n",
        "\n",
        "# 將張量 t13 位置 [[0, 1]] 的數值改成 10\n",
        "t13[[0, 1]] = -999\n",
        "print(t13)"
      ],
      "metadata": {
        "id": "GZIZQ8FjLMLV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 判斷式取值\n",
        "\n",
        "# 宣告 Tensor 變數\n",
        "t15 = torch.tensor([\n",
        "    0, 10, 20, 30, 40,\n",
        "    50, 60, 70, 80, 90\n",
        "])\n",
        "\n",
        "# 輸出每個值是否大於 50 的 `torch.Tensor`\n",
        "print(t15 > 50)\n",
        "# 輸出 torch.bool\n",
        "print((t15 > 50).dtype)\n",
        "# 輸出大於 50 的值 [60, 70, 80, 90]\n",
        "print(t15[t15 > 50])\n",
        "# 輸出除以 20 餘數為 0 的值 [0, 20, 40, 60, 80]\n",
        "print(t15[t15 % 20 == 0])"
      ],
      "metadata": {
        "id": "8ZSFsd9yIuoQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 張量運算\n",
        "\n",
        "### 純量運算（Scalar Operation）\n",
        "\n",
        "對張量內所有數值與單一純量（Scalar）進行相同計算。\n",
        "\n",
        "|符號|意義|\n",
        "|-|-|\n",
        "|`torch.Tensor + scalar`|張量中的每個數值加上 `scalar`|\n",
        "|`torch.Tensor - scalar`|張量中的每個數值減去 `scalar`|\n",
        "|`torch.Tensor * scalar`|張量中的每個數值乘上 `scalar`|\n",
        "|`torch.Tensor / scalar`|張量中的每個數值除以 `scalar`|\n",
        "|`torch.Tensor // scalar`|張量中的每個數值除以 `scalar` 所得之商|\n",
        "|`torch.Tensor % scalar`|張量中的每個數值除以 `scalar` 所得之餘數|\n",
        "|`torch.Tensor ** scalar`|張量中的每個數值取 `scalar` 次方|\n",
        "\n",
        "### 個別數值運算（Element-wise Operation）\n",
        "\n",
        "若兩個張量想要進行運算，則兩個張量的**維度必須相同**（即兩張量之 `torch.size()` 相同）。\n",
        "\n",
        "|符號|意義|\n",
        "|-|-|\n",
        "|`A + B`|張量 `A` 中的每個數值加上張量 `B` 中相同位置的數值|\n",
        "|`A - B`|張量 `A` 中的每個數值減去張量 `B` 中相同位置的數值|\n",
        "|`A * B`|張量 `A` 中的每個數值乘上張量 `B` 中相同位置的數值|\n",
        "|`A / B`|張量 `A` 中的每個數值除以張量 `B` 中相同位置的數值|\n",
        "|`A // B`|張量 `A` 中的每個數值除以張量 `B` 中相同位置的數值所得之商|\n",
        "|`A % B`|張量 `A` 中的每個數值除以張量 `B` 中相同位置的數值所得之餘數|\n",
        "|`A ** B`|張量 `A` 中的每個數值取張量 `B` 中相同位置的數值之次方|\n",
        "\n",
        "### 個別數值函數運算（Element-wise Functional Operation）\n",
        "\n",
        "若想對張量中的**所有數值**進行**相同函數運算**，必須透過 `torch` 提供的介面進行。\n",
        "\n",
        "|函數|意義|\n",
        "|-|-|\n",
        "|`torch.sin`|張量中的每個數值 $x$ 計算 $\\sin(x)$|\n",
        "|`torch.cos`|張量中的每個數值 $x$ 計算 $\\cos(x)$|\n",
        "|`torch.tan`|張量中的每個數值 $x$ 計算 $\\tan(x)$|\n",
        "|`torch.exp`|張量中的每個數值 $x$ 計算 $e^{x}$|\n",
        "|`torch.log`|張量中的每個數值 $x$ 計算 $\\log x$\n",
        "|`torch.ceil`|張量中的每個數值 $x$ 計算 $\\left\\lceil x \\right\\rceil$\n",
        "|`torch.floor`|張量中的每個數值 $x$ 計算 $\\left\\lfloor x \\right\\rfloor$"
      ],
      "metadata": {
        "id": "8H-IQZQtI1rO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 純量運算(Scalar Operation)\n",
        "\n",
        "# 宣告 Tensor 變數\n",
        "t16 = torch.tensor([\n",
        "    [0, 10, 20],\n",
        "    [30, 40, 50],\n",
        "    [60, 70, 80],\n",
        "    [90, 100, 110],\n",
        "])\n",
        "\n",
        "# 輸出張量 t16\n",
        "print(t16)\n",
        "print()\n",
        "# 對張量 t16 所有數值加 5\n",
        "print(t16 + 5)\n",
        "print()\n",
        "# 對張量 t16 所有數值減 4\n",
        "print(t16 - 4)\n",
        "print()\n",
        "# 對張量 t16 所有數值乘 3\n",
        "print(t16 * 3)\n",
        "print()\n",
        "# 對張量 t16 所有數值除以 10\n",
        "print(t16 / 10)\n",
        "print()\n",
        "# 對張量 t16 所有數值除以 10 所得整數部份\n",
        "print(t16 // 10)\n",
        "print()\n",
        "# 對張量 t16 所有數值除以 7 得到餘數\n",
        "print(t16 % 7)\n",
        "print()\n",
        "# 對張量 t16 所有數值取 2 次方\n",
        "print(t16 ** 2)"
      ],
      "metadata": {
        "id": "SQ3V-YrGIv9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 個別數值運算\n",
        "\n",
        "# 宣告 Tensor 變數\n",
        "t17 = torch.tensor([\n",
        "    [1, 2, 3],\n",
        "    [4, 5, 6]\n",
        "])\n",
        "\n",
        "# 宣告 Tensor 變數\n",
        "t18 = torch.tensor([\n",
        "    [6, 5, 4],\n",
        "    [3, 2, 1]\n",
        "])\n",
        "\n",
        "# 張量相加\n",
        "print(t17 + t18)\n",
        "print()\n",
        "# 張量相減\n",
        "print(t17 - t18)\n",
        "print()\n",
        "# 張量相乘\n",
        "print(t17 * t18)\n",
        "print()\n",
        "# 張量相除\n",
        "print(t17 / t18)\n",
        "print()\n",
        "# 張量相除取商\n",
        "print(t17 // t18)\n",
        "print()\n",
        "# 張量相除取餘數\n",
        "print(t17 % t18)\n",
        "print()\n",
        "# 張量 A 取張量 B 次方\n",
        "print(t17 ** t18)"
      ],
      "metadata": {
        "id": "gRU-GLdvJDFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 🚧 **張量自動擴充（Broadcasting)**\n",
        "\n",
        "若張量 `A` 的維度為 `(a1, a2, ..., an)`（即 `A.size() == (a1, a2, ..., an)`），則張量 `B` 在滿足以下其中一種條件時即可與張量 `A` 進行運算：\n",
        "\n",
        "- 張量 `B` 與張量 `A` 維度完全相同（即 `B.size() == (a1, a2, ..., an)`）\n",
        "- 張量 `B` 為純量（即 `B.size() == (1,)`）\n",
        "- 張量 `B` 的維度為 `(b1, b2, ..., bn)`，若 `ai != bi`，則 `ai == 1` 或 `bi == 1`\n",
        "    - 從**最後**一個維度開始比較\n",
        "    - 如果有任何一個維度無法滿足前述需求，則會得到 `ValueError`"
      ],
      "metadata": {
        "id": "kGJ7i4d5JVDs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 張量自動擴充\n",
        "\n",
        "# 宣告 Tensor 變數\n",
        "t19 = torch.tensor([\n",
        "    [\n",
        "        [1, 2],\n",
        "        [3, 4],\n",
        "        [5, 6],\n",
        "    ],\n",
        "    [\n",
        "        [7, 8],\n",
        "        [9 ,10],\n",
        "        [11, 12]\n",
        "    ]\n",
        "])\n",
        "\n",
        "# 宣告 Tensor 變數\n",
        "t20 = torch.tensor([\n",
        "    [\n",
        "        [1],\n",
        "        [1],\n",
        "        [1]\n",
        "    ],\n",
        "    [\n",
        "        [2],\n",
        "        [2],\n",
        "        [2]\n",
        "    ],\n",
        "])\n",
        "\n",
        "# 輸出張量 t19 維度\n",
        "print(t19.size())\n",
        "# 輸出張量 t20 維度\n",
        "print(t20.size())\n",
        "print()\n",
        "# 張量 t19 與張量 t19 維度相同，所以可以直接運算\n",
        "print(t19 + t19)\n",
        "print()\n",
        "# 張量 t19 與張量 t20 可以擴充成相同維度，所以可以運算\n",
        "print(t19 + t20)"
      ],
      "metadata": {
        "id": "AduYqrAOJUDR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51548797-115d-4a37-bc47-5199d90b0bb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3, 2])\n",
            "torch.Size([2, 3, 1])\n",
            "\n",
            "tensor([[[ 2,  4],\n",
            "         [ 6,  8],\n",
            "         [10, 12]],\n",
            "\n",
            "        [[14, 16],\n",
            "         [18, 20],\n",
            "         [22, 24]]])\n",
            "\n",
            "tensor([[[ 2,  3],\n",
            "         [ 4,  5],\n",
            "         [ 6,  7]],\n",
            "\n",
            "        [[ 9, 10],\n",
            "         [11, 12],\n",
            "         [13, 14]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 張量 t17 與張量 t18 可以擴充成相同維度，所以可以運算\n",
        "print(t19 + t20)\n",
        "t20_like = torch.tensor([\n",
        "    [\n",
        "        [1, 1],\n",
        "        [1, 1],\n",
        "        [1, 1]\n",
        "    ],\n",
        "    [\n",
        "        [2, 2],\n",
        "        [2, 2],\n",
        "        [2, 2]\n",
        "    ],\n",
        "])\n",
        "print()\n",
        "\n",
        "print(t20_like.size())\n",
        "print()\n",
        "\n",
        "# 輸出 True，因為 t20_like 與 t20 擴充後維度相同\n",
        "print(torch.equal(t20 + t19, t20_like + t19))"
      ],
      "metadata": {
        "id": "1kB2F2drJh_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 賦值（Assignment）\n",
        "\n",
        "使用 `=` 賦與指定位置數值。可以使用 `iterable` 一次指定多個位置。\n",
        "\n",
        "|符號|意義|\n",
        "|-|-|\n",
        "|`=`|賦值|\n",
        "|`+=`|進行加法後賦值|\n",
        "|`-=`|進行減法後賦值|\n",
        "|`*=`|進行乘法後賦值|"
      ],
      "metadata": {
        "id": "SA76gTooMudB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 使用 = 的賦值操作\n",
        "\n",
        "t21 = torch.tensor([\n",
        "    [1, 2, 3],\n",
        "    [4, 5, 6],\n",
        "    [7, 8, 9],\n",
        "    [10, 11, 12]\n",
        "])\n",
        "print(t21)\n",
        "print()\n",
        "\n",
        "# 將張量 t21 位置 0 的所有數值加上 1995\n",
        "t21[0] += 1995\n",
        "print(t21)\n",
        "print()\n",
        "\n",
        "# 將張量 t21 位置 [0, 1] 的所有數值減掉 10\n",
        "t21[0, 1] -= 10\n",
        "print(t21)\n",
        "print()\n",
        "\n",
        "# 將張量 t21 位置 [2, 1] 與 [0, 2] 的所有數值乘上 12\n",
        "t21[[2, 0], [1, 2]] *= 12\n",
        "print(t21)"
      ],
      "metadata": {
        "id": "8tWnVi6FMtsC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 高維張量運算\n",
        "矩陣等同於是維度為 2 的張量。\n",
        "而高維度的張量運算等同於**固定大部分的維度**，只使用**其中的兩個維度進行計算**。\n",
        "\n",
        "### 張量乘法（Tensor Multiplication）\n",
        "\n",
        "例如：以 $A.\\text{size}() = (4, 3)$ 與 $B.\\text{size}() = (3, 2)$ 來說，$(A \\times B).\\text{size}() = (4, 2)$。\n",
        "\n",
        "例如：以 $A.\\text{size}() = (5, 4, 3)$ 與 $B.\\text{size}() = (5, 3, 2)$ 來說，$(A \\times B).\\text{size}() = (5, 4, 2)$。\n",
        "\n",
        "例如：以 $A.\\text{size}() = (1995, 10, 12, 5, 4, 3)$ 與 $B.\\text{size}() = (1995, 10, 12, 5, 3, 2)$ 來說，$(A \\times B).\\text{size}() = (1995, 10, 12, 5, 4, 2)$。"
      ],
      "metadata": {
        "id": "9am_O03fOKcN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 張量乘法\n",
        "\n",
        "# 宣告 Tensor 變數\n",
        "t22 = torch.ones(5, 4, 3)\n",
        "# 宣告 Tensor 變數\n",
        "t23 = torch.ones(5, 3, 2)\n",
        "# 進行張量乘法\n",
        "t24 = torch.matmul(t22, t23)\n",
        "\n",
        "# 輸出張量 t22 的維度\n",
        "print(t22.size())\n",
        "# 輸出張量 t23 的維度\n",
        "print(t23.size())\n",
        "# 輸出張量 t24 的維度\n",
        "print(t24.size())"
      ],
      "metadata": {
        "id": "3gZxuFStOKwS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50960f9d-005c-4442-820b-953d629e4f82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([5, 4, 3])\n",
            "torch.Size([5, 3, 2])\n",
            "torch.Size([5, 4, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 矩陣乘法\n",
        "\n",
        "# 宣告 Tensor 變數\n",
        "t25 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
        "\n",
        "t26 = torch.full((3,2), 2)\n",
        "print(t26)\n",
        "print()\n",
        "\n",
        "t27 = torch.matmul(t25, t26)\n",
        "print(t27)\n",
        "print(t25.size())\n",
        "print(t26.size())\n",
        "print(t27.size())"
      ],
      "metadata": {
        "id": "9Hf8W2BTPYsB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d602413-b564-4f11-b3c3-3f3cb4d57073"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[2, 2],\n",
            "        [2, 2],\n",
            "        [2, 2]])\n",
            "\n",
            "tensor([[12, 12],\n",
            "        [30, 30]])\n",
            "torch.Size([2, 3])\n",
            "torch.Size([3, 2])\n",
            "torch.Size([2, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 張量轉置\n",
        "\n",
        "# 宣告 Tensor 變數\n",
        "t28 = torch.ones(5, 4, 3)\n",
        "\n",
        "# 輸出轉置維度 1 與 2 後的維度\n",
        "print(torch.transpose(t28, 1, 2).size())\n",
        "# 輸出轉置維度 1 與 2 後的維度\n",
        "print(t28.transpose(1, 2).size())\n",
        "\n",
        "# 輸出轉置維度 0 與 2 後的維度\n",
        "print(torch.transpose(t28, 0, 2).size())\n",
        "# 輸出轉置維度 0 與 2 後的維度\n",
        "print(t28.transpose(0, 2).size())"
      ],
      "metadata": {
        "id": "jzmMzivAORsv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f222bb5b-0bfb-427a-e557-2e83749793ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([5, 3, 4])\n",
            "torch.Size([5, 3, 4])\n",
            "torch.Size([3, 4, 5])\n",
            "torch.Size([3, 4, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## view 與 reshape\n",
        "[`view()`](https://pytorch.org/docs/stable/generated/torch.Tensor.view.html#torch-tensor-view) 與 [`reshape()`](https://pytorch.org/docs/stable/generated/torch.reshape.html#torch.reshape) 都可以用來改變 Tensor 的形狀，但它們有一些關鍵的區別：\n",
        "- 記憶體連續性（Memory Contiguity）\n",
        "    - `view()` 要求原 Tensor 在記憶體中是連續的（contiguous），否則會報錯。\n",
        "    - `reshape()` 不強制要求記憶體連續，如果 Tensor 不是 contiguous 的，它會自動調用 .contiguous() 後再重新排列數據。\n",
        "\n",
        "- 內部實現\n",
        "    - `view()` 不會改變資料本身，只是改變視圖（view），它返回的是與原 Tensor 共享資料的 Tensor。\n",
        "    - `reshape()` 可能會返回原資料的視圖，也可能會創建新的 Tensor 並複製資料，具體取決於記憶體分配。"
      ],
      "metadata": {
        "id": "APVMu9OjQE8C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 宣告 Tensor 變數\n",
        "t29 = torch.tensor([\n",
        "    [1, 2, 3],\n",
        "    [4, 5, 6],\n",
        "    [7, 8, 9],\n",
        "    [10, 11, 12],\n",
        "])\n",
        "print(t29.transpose(1, 0).is_contiguous())"
      ],
      "metadata": {
        "id": "eSrxR0zsQKVF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# t29 是一個 4x3 的矩陣，可以以 view() 轉換為 3x4\n",
        "t29.view(3,4)"
      ],
      "metadata": {
        "id": "zKFYIf5jScq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 經過 transpose 後，t29 的 tensor 在記憶體中的排列就不 contiguous\n",
        "# view() 只接受 contiguous 的 Tensor，因此，此 cell 會報錯\n",
        "\n",
        "t29.transpose(1, 0).view(4,3)"
      ],
      "metadata": {
        "id": "GryH9-DJShlg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reshape() 則可以接受 contiguous 的 Tensor\n",
        "\n",
        "t29.transpose(1, 0).reshape(4,3)"
      ],
      "metadata": {
        "id": "_JzbS0aBUmL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# .contiguous()\n",
        "# 經過 transpose 後，t29 的 tensor 在記憶體中的排列雖然不 contiguous，但可以用 .contiguous() 來恢復在記憶中中連續排列的特性\n",
        "\n",
        "t29.transpose(1, 0).contiguous().view(4,3)"
      ],
      "metadata": {
        "id": "VGk6i1g6SkDr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "GKhaAs1MUh5r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 維度運算\n",
        "\n",
        "### 降維函數（Dimension Decreasing Function）\n",
        "\n",
        "以下函數將會使**輸出**張量維度**小於輸入**張量維度。\n",
        "在機器學習中你幾乎必定會用到 `torch.argmax()`。\n",
        "\n",
        "|函數|意義|\n",
        "|-|-|\n",
        "|`torch.sum`|將所有數值相加|\n",
        "|`torch.max`|取出所有數值中最大者|\n",
        "|`torch.min`|取出所有數值中最小者|\n",
        "|`torch.argmax`|取出所有數值中最大者的位置|\n",
        "|`torch.argmin`|取出所有數值中最小者的位置|\n",
        "|`torch.mean`|取出所有數值的平均值|\n",
        "|`torch.var`|取出所有數值的變異數|\n",
        "|`torch.std`|取出所有數值的標準差|\n",
        "|`torch.squeeze`|移除數字為 1 的維度|\n",
        "\n",
        "### 增維函數（Dimension Increasing Function）\n",
        "\n",
        "以下函數將會使**輸出**張量維度**大於輸入**張量維度。\n",
        "\n",
        "|函數|意義|\n",
        "|-|-|\n",
        "|`torch.cat`|串接多個相同維度的張量|\n",
        "|`torch.unsqueeze`|在指定的維度間增加 1 維度|"
      ],
      "metadata": {
        "id": "tOIxv0QkVFcD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "fUFCCH_J4EoW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 降維函數\n",
        "\n",
        "# 宣告 Tensor 變數\n",
        "t30 = torch.tensor([\n",
        "    [1, 2, 3, 4],\n",
        "    [5, 6, 7, 8],\n",
        "    [9, 10, 11, 12]\n",
        "])\n",
        "\n",
        "# 將張量 t30 中所有值相加\n",
        "print(torch.sum(t30))\n",
        "# 將張量 t30 中所有值相加\n",
        "print(t30.sum())\n",
        "\n",
        "# 將張量 t30 中依照維度 0 將所有值相加\n",
        "print(torch.sum(t30, dim=0))\n",
        "# 將張量 t30 中依照維度 0 將所有值相加\n",
        "print(t30.sum(dim=0))\n",
        "# 將張量 t30 中依照維度 1 將所有值相加\n",
        "print(torch.sum(t30, dim=1))\n",
        "# 將張量 t30 中依照維度 1 將所有值相加\n",
        "print(t30.sum(dim=1))\n",
        "\n",
        "# 找出張量 t30 中最大值\n",
        "print(torch.max(t30))\n",
        "# 找出張量 t30 中最大值\n",
        "print(t30.max())\n",
        "print()\n",
        "\n",
        "# 依照維度 0 找出張量 t30 中最大值，並回傳最大值與對應位置\n",
        "print(torch.max(t30, dim=0))\n",
        "print()\n",
        "# 依照維度 0 找出張量 t30 中最大值，並回傳最大值與對應位置\n",
        "print(t30.max(dim=0))\n",
        "print()\n",
        "# 依照維度 0 找出張量 t30 中最大值\n",
        "print(torch.max(t30, dim=0)[0])\n",
        "# 依照維度 0 找出張量 t30 中最大值位置\n",
        "print(torch.max(t30, dim=0)[1])\n",
        "print()\n",
        "\n",
        "# 找出張量 t30 中最小值\n",
        "print(torch.min(t30))\n",
        "# 找出張量 t30 中最小值\n",
        "print(t30.min())\n",
        "print()\n",
        "\n",
        "# 依照維度 1 找出張量 t30 中最小值，並回傳最小值與對應位置\n",
        "print(torch.min(t30, dim=1))\n",
        "print()\n",
        "# 依照維度 1 找出張量 t30 中最小值，並回傳最小值與對應位置\n",
        "print(t30.min(dim=1))\n",
        "print()\n",
        "# 依照維度 1 找出張量 t30 中最小值\n",
        "print(torch.min(t30, dim=1)[0])\n",
        "# 依照維度 1 找出張量 t30 中最小值位置\n",
        "print(torch.min(t30, dim=1)[1])"
      ],
      "metadata": {
        "id": "EeWgNqATVHM0",
        "outputId": "80d5db4b-4450-4bfc-c183-699011077a0a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(78)\n",
            "tensor(78)\n",
            "tensor([15, 18, 21, 24])\n",
            "tensor([15, 18, 21, 24])\n",
            "tensor([10, 26, 42])\n",
            "tensor([10, 26, 42])\n",
            "tensor(12)\n",
            "tensor(12)\n",
            "\n",
            "torch.return_types.max(\n",
            "values=tensor([ 9, 10, 11, 12]),\n",
            "indices=tensor([2, 2, 2, 2]))\n",
            "\n",
            "torch.return_types.max(\n",
            "values=tensor([ 9, 10, 11, 12]),\n",
            "indices=tensor([2, 2, 2, 2]))\n",
            "\n",
            "tensor([ 9, 10, 11, 12])\n",
            "tensor([2, 2, 2, 2])\n",
            "\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "\n",
            "torch.return_types.min(\n",
            "values=tensor([1, 5, 9]),\n",
            "indices=tensor([0, 0, 0]))\n",
            "\n",
            "torch.return_types.min(\n",
            "values=tensor([1, 5, 9]),\n",
            "indices=tensor([0, 0, 0]))\n",
            "\n",
            "tensor([1, 5, 9])\n",
            "tensor([0, 0, 0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# argmax & argmin\n",
        "\n",
        "# 宣告 Tensor 變數\n",
        "t31 = torch.tensor([\n",
        "    [1, 2, 3, 4],\n",
        "    [5, 6, 7, 8],\n",
        "    [9, 10, 11, 12]\n",
        "])\n",
        "print('shape:', t31.shape)\n",
        "print('=== argmax ===')\n",
        "# 找出張量 t31 中最大值的位置\n",
        "print('- flattened argmax:')\n",
        "print(torch.argmax(t31))\n",
        "# 找出張量 t31 中最大值的位置\n",
        "# dim (int) – the dimension to reduce. If None, the argmax of the flattened input is returned.\n",
        "print(t31.argmax())\n",
        "print(\"- reduced along dim 0:\")\n",
        "# 依照維度 0 找出張量 t31 中最大值的位置，\n",
        "# 維度 0: dimension TO REDUCE，代表沿著維度 0 會被攤平，shape (3,4) 的 tensor 會變成 shape (4,) 的 tensor\n",
        "# 沿著 [1,5,9], [2, 6, 10], ... 找最大值的 index。\n",
        "print(torch.argmax(t31, dim=0))\n",
        "# 依照維度 0 找出張量 t31 中最大值的位置\n",
        "print(t31.argmax(dim=0))\n",
        "\n",
        "print(\"- reduced along dim 1:\")\n",
        "# 依照維度 1 找出張量 t31 中最大值的位置\n",
        "print(torch.argmax(t31, dim=1))\n",
        "# 依照維度 1 找出張量 t31 中最大值的位置\n",
        "print(t31.argmax(dim=1))\n",
        "\n",
        "print('=== argmin ===')\n",
        "# 找出張量 t31 中最小值的位置\n",
        "print(torch.argmin(t31))\n",
        "# 找出張量 t31 中最小值的位置\n",
        "print(t31.argmin())"
      ],
      "metadata": {
        "id": "CkCSF2ueVa09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 宣告 Tensor 變數\n",
        "t32 = torch.tensor([\n",
        "    [1., 2., 3., 4.],\n",
        "    [5., 6., 7., 8.],\n",
        "    [9., 10., 11., 12.]\n",
        "])\n",
        "\n",
        "# 計算張量 t32 中所有值的平均數\n",
        "print(torch.mean(t32))\n",
        "# 計算張量 t32 中所有值的平均數\n",
        "print(t32.mean())\n",
        "\n",
        "# 依照維度 0 計算張量 t32 中所有值的平均數\n",
        "print(torch.mean(t32, axis=0))\n",
        "# 依照維度 0 計算張量 t32 中所有值的平均數\n",
        "print(t32.mean(axis=0))\n",
        "# 依照維度 1 計算張量 t32 中所有值的平均數\n",
        "print(torch.mean(t32, axis=1))\n",
        "# 依照維度 1 計算張量 t32 中所有值的平均數\n",
        "print(t32.mean(axis=1))\n",
        "\n",
        "# 計算張量 t32 中所有值的變異數\n",
        "print(torch.var(t32))\n",
        "# 計算張量 t32 中所有值的變異數\n",
        "print(t32.var())\n",
        "\n",
        "# 依照維度 0 計算張量 t32 中所有值的變異數\n",
        "print(torch.var(t32, axis=0))\n",
        "# 依照維度 0 計算張量 t32 中所有值的變異數\n",
        "print(t32.var(axis=0))\n",
        "# 依照維度 1 計算張量 t32 中所有值的變異數\n",
        "print(torch.var(t32, axis=1))\n",
        "# 依照維度 1 計算張量 t32 中所有值的變異數\n",
        "print(t32.var(axis=1))\n",
        "\n",
        "# 計算張量 t32 中所有值的標準差\n",
        "print(torch.std(t32))\n",
        "# 計算張量 t32 中所有值的標準差\n",
        "print(t32.std())\n",
        "\n",
        "# 依照維度 0 計算張量 t32 中所有值的標準差\n",
        "print(torch.std(t32, axis=0))\n",
        "# 依照維度 0 計算張量 t32 中所有值的標準差\n",
        "print(t32.std(axis=0))\n",
        "# 依照維度 1 計算張量 t32 中所有值的標準差\n",
        "print(torch.std(t32, axis=1))\n",
        "# 依照維度 1 計算張量 t32 中所有值的標準差\n",
        "print(t32.std(axis=1))"
      ],
      "metadata": {
        "id": "sI-XbYmFVrBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 宣告 Tensor 變數\n",
        "t33 = torch.tensor([\n",
        "    [1, 2, 3]\n",
        "])\n",
        "\n",
        "# 移除張量 t33 中多餘的維度(為 1 的維度)\n",
        "t33_sq = torch.squeeze(t33)\n",
        "\n",
        "# 輸出張量 t33\n",
        "print(t33)\n",
        "# 輸出張量 t33 的維度\n",
        "print('- before squeezing:', t33.size())\n",
        "# 輸出移除維度後的張量 t33\n",
        "print(t33_sq)\n",
        "# 輸出移除維度後張量 t33 的維度\n",
        "print('- after squeezing:',t33_sq.size())"
      ],
      "metadata": {
        "id": "YhqfXmiAWLlS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 增維函數 (torch.cat)\n",
        "\n",
        "# 宣告 Tensor 變數\n",
        "t34 = torch.tensor([\n",
        "    [1, 2, 3]\n",
        "])\n",
        "\n",
        "# 串接多個張量 t34\n",
        "t34_cat = torch.cat([\n",
        "    t34,\n",
        "    t34,\n",
        "    t34,\n",
        "    t34\n",
        "])\n",
        "\n",
        "# 輸出串接後的張量 t34_cat\n",
        "print(t34_cat)\n",
        "# 輸出串接後的張量 t34_cat 維度\n",
        "print(t34_cat.size())"
      ],
      "metadata": {
        "id": "qq28sG7EV07j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 增維函數 (torch.unsqueeze)\n",
        "\n",
        "# 宣告 Tensor 變數\n",
        "t35 = torch.tensor([\n",
        "    [1, 2, 3],\n",
        "    [4, 5, 6]\n",
        "])\n",
        "\n",
        "print(t35)\n",
        "print(t35.size())\n",
        "\n",
        "# 對張量 t35 維度 0 增加 1 維\n",
        "t35_usq = torch.unsqueeze(t35, dim=0)\n",
        "\n",
        "# 輸出張量 t35 維度 0 增加 1 維後的結果\n",
        "print(t35_usq)\n",
        "# 輸出張量 t35 維度 0 增加 1 維後的維度\n",
        "print(t35_usq.size())"
      ],
      "metadata": {
        "id": "yWBCDXnzWQ-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 使用 GPU 運算\n",
        "\n",
        "上述的所有教學都是在 CPU 上進行運算，而大多數的深度學習框架都會提供操作 GPU 的介面幫助平型化運算。\n",
        "而 `torch` 與大部分的深度學習框架相同，使用 Nvidia 開發的 CUDA（Compute Unified Device Architecture）幫助使用 GPU 進行深度學習的運算（cuDNN）。\n",
        "\n",
        "使用 CUDA 操作平型化運算的流程為：\n",
        "\n",
        "1. 宣告 GPU 運算所需要佔用的記憶體（`cudaMalloc`）\n",
        "2. 定義每個平型化運算節點的運算內容\n",
        "3. 在主記憶體上創造資料（`malloc`）\n",
        "4. 將資料搬移至 GPU 的記憶體（`cudaMemcpy`）\n",
        "5. 每個節點獨立運算\n",
        "6. 將計算結果搬回至主記憶體（`memcpy`）\n",
        "7. 釋放 GPU 的記憶體（`cudaFree`）\n",
        "\n",
        "而在 `torch` 中將以上流程簡化成以下兩種方法\n",
        "\n",
        "- 宣告 `torch.Tensor` 變數時使用 `device='cuda:0'` 參數將變數宣告於 GPU 記憶體第0顆（0-indexed）。\n",
        "- 對已經創造於主記憶體的 `torch.Tensor` 變數使用 `torch.to('cuda:0')` 搬移至 GPU 記憶體第0顆（0-indexed）。\n",
        "```python\n",
        "torch.tensor([1., 2., 3.], device='cuda:0') # 使用 device 參數將變數宣告於 GPU 記憶體\n",
        "torch.tensor([1., 2., 3.]).to('cuda:0')     # 使用 to 將變數搬移至 GPU 記憶體\n",
        "```\n",
        "\n",
        "宣告於 GPU 或搬移至 GPU 後，之後所有的運算便會在 GPU 上進行。"
      ],
      "metadata": {
        "id": "n7I2PjqTWql1"
      }
    }
  ]
}