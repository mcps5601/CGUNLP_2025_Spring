{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rf15Dk3u2yTp"
      },
      "source": [
        "# Document retrieval with TF-IDF\n",
        "The goal of this notebook is to implement a document retrieval system  using the [`TF-IDF`](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPy_Anrb2yTq"
      },
      "source": [
        "## What is TF-IDF?\n",
        "- TF: Term Frequency, the occurring time ($n$) of `a term` $t$ appears in `a document` $d$.\n",
        "$$\\textrm{TF}_{t,d} =  \\frac{n_{t,d}}{\\sum_k n_{k,d}}$$\n",
        ", where $\\sum_k n_{k,d}$ indicates summation of all terms in the document $d$.\n",
        "- IDF: Inverse Document Frequency, how frequent `a term` $t$ appears in all documents.\n",
        "    - Document Frequency $\\textrm{DF}_t = \\log\\frac{|\\{d: t\\in d\\}|}{|D|}$, where $|\\{d: t\\in d\\}|$ is the number of documents that `a term` t appears, and $|D|$ indicates total number of documents.\n",
        "    - $\\Rightarrow \\textrm{IDF}_t = \\log\\frac{|D|}{|\\{d: t\\in d\\}|}$\n",
        "- $\\textrm{TFIDF}_{t,d} = \\textrm{TF}_{t,d} \\times \\textrm{IDF}_t$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkI-ztQs2yTq"
      },
      "source": [
        "## What can TF-IDF do?\n",
        "- TF-IDF can be used to transform a document or a sentence into a `vector`.\n",
        "- There are other ways to transform a document or a sentence into a vector, such as [`word2vec`](https://radimrehurek.com/gensim/models/word2vec.html), [`BERT`](https://huggingface.co/docs/transformers/training), [`Sentence-BERT`](https://github.com/UKPLab/sentence-transformers), etc.\n",
        "- **This homework only asks you to implement the TF-IDF method.** You can try other methods in your final project."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgPXlMGq2yTr"
      },
      "source": [
        "## Example of TF-IDF using `scikit-learn`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5-WyLiq2yTr"
      },
      "source": [
        "### Example 1: Use [`CountVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) + [`TfidfTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html)\n",
        "```python\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Assume we have 4 documents in a list\n",
        "corpus = [\n",
        "    \"this is the first document\",\n",
        "    \"this document is the second document\",\n",
        "    \"and this is the third one\",\n",
        "    \"is this the first document\",\n",
        "]\n",
        "# Assume we want some important words in the vocabulary\n",
        "vocabulary = [\"this\", \"document\", \"first\", \"is\", \"second\", \"the\", \"and\", \"one\"]\n",
        "pipe = Pipeline(\n",
        "    [\n",
        "        (\"count\", CountVectorizer(vocabulary=vocabulary)),  # Get the count of each word\n",
        "        (\"tfidf\", TfidfTransformer(norm=None)),  # Get the tfidf of each word\n",
        "    ]\n",
        ").fit(corpus)\n",
        "\n",
        "# Print the count of each word\n",
        "print(pipe[\"count\"].transform(corpus).toarray())\n",
        ">>> [[1, 1, 1, 1, 0, 1, 0, 0], # count of the first document\n",
        "     [1, 2, 0, 1, 1, 1, 0, 0], # count of the second document\n",
        "     [1, 0, 0, 1, 0, 1, 1, 1], # count of the third document\n",
        "     [1, 1, 1, 1, 0, 1, 0, 0]] # count of the fourth document\n",
        "\n",
        "# Print the IDF values of each word\n",
        "# Notice!! The shape of the IDF values is (n_features, ).\n",
        "# n_features is the number of words in the vocabulary.\n",
        "print(pipe[\"tfidf\"].idf_)\n",
        ">>> [1.         1.22314355 1.51082562 1.         1.91629073 1.\n",
        " 1.91629073 1.91629073]\n",
        "\n",
        "# Print the TF-IDF values\n",
        "# Notice!! The shape of the TF-IDF values is (n_documents, n_features).\n",
        "print(pipe.transform(corpus).toarray())\n",
        ">>> [[1.         1.22314355 1.51082562 1.         0.         1.\n",
        "  0.         0.        ]\n",
        " [1.         2.4462871  0.         1.         1.91629073 1.\n",
        "  0.         0.        ]\n",
        " [1.         0.         0.         1.         0.         1.\n",
        "  1.91629073 1.91629073]\n",
        " [1.         1.22314355 1.51082562 1.         0.         1.\n",
        "  0.         0.        ]]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHmTFqcK2yTr"
      },
      "source": [
        "### Example 2: Use [`TfidfVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) alone\n",
        "```python\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Assume we have 4 documents in a list\n",
        "corpus = [\n",
        "    \"this is the first document\",\n",
        "    \"this document is the second document\",\n",
        "    \"and this is the third one\",\n",
        "    \"is this the first document\",\n",
        "]\n",
        "# Assume we want some important words in the vocabulary\n",
        "vocabulary = [\"this\", \"document\", \"first\", \"is\", \"second\", \"the\", \"and\", \"one\"]\n",
        "vectorizer = TfidfVectorizer(\n",
        "    vocabulary=vocabulary,\n",
        "    norm=None, # without normalization\n",
        ")\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "print(X.toarray())\n",
        ">>> [[1.         1.22314355 1.51082562 1.         0.         1.\n",
        "  0.         0.        ]\n",
        " [1.         2.4462871  0.         1.         1.91629073 1.\n",
        "  0.         0.        ]\n",
        " [1.         0.         0.         1.         0.         1.\n",
        "  1.91629073 1.91629073]\n",
        " [1.         1.22314355 1.51082562 1.         0.         1.\n",
        "  0.         0.        ]]\n",
        "```\n",
        "- Example 2 is the same as Example 1, but we use `TfidfVectorizer` instead of `CountVectorizer` + `TfidfTransformer`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5JVcvuL2yTr"
      },
      "source": [
        "## Why do we need to transform a document or a sentence into a vector?\n",
        "- Because we can use the `cosine similarity` to measure the similarity between each sentence and a Wikipedia article."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvImq4HY2yTs"
      },
      "source": [
        "### Folder structure\n",
        "- Please follow the following folder structure.\n",
        "```bash\n",
        "./\n",
        "│\n",
        "├── data/\n",
        "│ ├── train.jsonl # Dataset you need to download and upload to Colab\n",
        "│ └── wiki-pages # Wiki data you need to download\n",
        "│\n",
        "├── TFIDF_doc_retrieval.ipynb # This file\n",
        "├── utils.py # helper functions\n",
        "└── requirements.txt\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iYop0bJc2yTs"
      },
      "outputs": [],
      "source": [
        "# Uncomment the following lines if you use Google Colab\n",
        "!pip install pandarallel\n",
        "!pip install TCSP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PZLf1MF42yTs"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from functools import partial\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import jieba\n",
        "import scipy\n",
        "\n",
        "# jieba.set_dictionary(\"dict.txt.big\")\n",
        "# Download \"dict.txt.big\" from https://github.com/fxsjy/jieba\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "from pandarallel import pandarallel\n",
        "# Adjust the number of workers if you want\n",
        "pandarallel.initialize(progress_bar=True, verbose=0, nb_workers=4)\n",
        "\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas() # for progress_apply\n",
        "\n",
        "from utils import (\n",
        "    load_json,\n",
        "    jsonl_dir_to_df,\n",
        "    calculate_precision,\n",
        "    calculate_recall,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yeQ1KnCo2yTt"
      },
      "outputs": [],
      "source": [
        "# Get the stopwords\n",
        "# https://github.com/bryanchw/Traditional-Chinese-Stopwords-and-Punctuations-Library\n",
        "from TCSP import read_stopwords_list\n",
        "\n",
        "stopwords = read_stopwords_list()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jjUUjqTO2yTt"
      },
      "outputs": [],
      "source": [
        "def tokenize(text: str, stopwords: list) -> str:\n",
        "    \"\"\"This function performs Chinese word segmentation and removes stopwords.\n",
        "\n",
        "    Args:\n",
        "        text (str): claim or wikipedia article\n",
        "        stopwords (list): common words that contribute little to the meaning of a sentence\n",
        "\n",
        "    Returns:\n",
        "        str: word segments separated by space (e.g. \"我 喜歡 吃 蘋果\")\n",
        "    \"\"\"\n",
        "\n",
        "    tokens = jieba.cut(text)\n",
        "\n",
        "    return \" \".join([w for w in tokens if w not in stopwords])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8KXLmv3t2yTt"
      },
      "outputs": [],
      "source": [
        "def get_pred_docs_sklearn(\n",
        "    claim: str,\n",
        "    tokenizing_method: callable,\n",
        "    vectorizer: TfidfVectorizer,\n",
        "    tf_idf_matrix: scipy.sparse.csr_matrix,\n",
        "    wiki_pages: pd.DataFrame,\n",
        "    topk: int,\n",
        ") -> set:\n",
        "\n",
        "    tokens = tokenizing_method(claim)\n",
        "    claim_vector = vectorizer.transform([tokens])\n",
        "    similarity_scores = cosine_similarity(tf_idf_matrix, claim_vector)\n",
        "\n",
        "    # `similarity_scores` shape: (num_wiki_pages x 1)\n",
        "    similarity_scores = similarity_scores[:, 0]  # flatten the array\n",
        "\n",
        "    # Sort the similarity scores in descending order\n",
        "    sorted_indices = np.argsort(similarity_scores)[::-1]\n",
        "    topk_sorted_indices = sorted_indices[:topk]\n",
        "\n",
        "    # Get the wiki page names based on the topk sorted indices\n",
        "    results = wiki_pages.iloc[topk_sorted_indices][\"id\"]\n",
        "\n",
        "    exact_matchs = []\n",
        "    # You can find the following code in our AICUP2023 baseline.\n",
        "    # Basically, we check if a result is exactly mentioned in the claim.\n",
        "    for result in results:\n",
        "        if (\n",
        "            (result in claim)\n",
        "            or (result in claim.replace(\" \", \"\")) # E.g., MS DOS -> MSDOS\n",
        "            or (result.replace(\"·\", \"\") in claim) # E.g., 湯姆·克魯斯 -> 湯姆克魯斯\n",
        "            or (result.replace(\"-\", \"\") in claim) # E.g., X-SAMPA -> XSAMPA\n",
        "        ):\n",
        "            exact_matchs.append(result)\n",
        "        elif \"·\" in result:\n",
        "            splitted = result.split(\"·\") # E.g., 阿爾伯特·愛因斯坦 -> 愛因斯坦\n",
        "            for split in splitted:\n",
        "                if split in claim:\n",
        "                    exact_matchs.append(result)\n",
        "                    break\n",
        "\n",
        "    return set(exact_matchs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rcjTA6N-2yTt"
      },
      "outputs": [],
      "source": [
        "# Helper function (you don't need to modify this)\n",
        "\n",
        "def get_title_from_evidence(evidence):\n",
        "    titles = []\n",
        "    for evidence_set in evidence:\n",
        "        if len(evidence_set) == 4 and evidence_set[2] is None:\n",
        "            return [None]\n",
        "        for evidence_sent in evidence_set:\n",
        "            titles.append(evidence_sent[2])\n",
        "    return list(set(titles))\n",
        "\n",
        "\n",
        "def save_results_to_markdown(results: dict, output_file=\"grid_search_results.md\"):\n",
        "    file_exists = Path(output_file).exists()\n",
        "\n",
        "    with open(output_file, \"a\") as f:\n",
        "        if not file_exists:\n",
        "            f.write(\"# Grid Search Results\\n\\n\")\n",
        "            f.write(\"| Experiment  | F1 Score | Precision | Recall |\\n\")\n",
        "            f.write(\"| ----------- | -------- | --------- | ------ | \\n\")\n",
        "\n",
        "        exp_name = results[\"exp_name\"]\n",
        "        f1 = results[\"f1_score\"]\n",
        "        prec = results[\"precision\"]\n",
        "        recall = results[\"recall\"]\n",
        "        f.write(f\"| {exp_name} | {f1:.4f} | {prec:.4f} | {recall:.4f} |\\n\")\n",
        "    print(f\"Results saved to {output_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oj4d9rhI2yTt"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "\n",
        "wiki_path = \"data/wiki-pages\"\n",
        "min_wiki_length = 10\n",
        "num_of_samples = 500\n",
        "topk = 15\n",
        "min_df = 2\n",
        "max_df = 0.8\n",
        "use_idf = True\n",
        "sublinear_tf = True\n",
        "\n",
        "# Set up the experiment name for logging\n",
        "exp_name = (\n",
        "    f\"len{min_wiki_length}_top{topk}_min_df={min_df}_\"\n",
        "    + f\"max_df={max_df}_{num_of_samples}s\"\n",
        ")\n",
        "if sublinear_tf:\n",
        "    exp_name = \"sublinearTF_\" + exp_name\n",
        "if not use_idf:\n",
        "    exp_name = \"no_idf_\" + exp_name"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id 1uFkoHbJ2iqm2pMR3rHHBTym3Q7pukKm8 -O wiki-pages.zip"
      ],
      "metadata": {
        "id": "7yeTG5k339vD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip data/wiki-pages.zip -d data/"
      ],
      "metadata": {
        "id": "NNWRfvA-4Hsm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VrHluL6m2yTt"
      },
      "outputs": [],
      "source": [
        "# First time running this cell will 34 minutes using Google Colab.\n",
        "\n",
        "wiki_cache = \"wiki\"\n",
        "target_column = \"text\"\n",
        "\n",
        "wiki_cache_path = Path(f\"data/{wiki_cache}.pkl\")\n",
        "if wiki_cache_path.exists():\n",
        "    wiki_pages = pd.read_pickle(wiki_cache_path)\n",
        "else:\n",
        "    # You need to download `wiki-pages.zip` from the AICUP website\n",
        "    wiki_pages = jsonl_dir_to_df(wiki_path)\n",
        "    # wiki_pages are combined into one dataframe, so we need to reset the index\n",
        "    wiki_pages = wiki_pages.reset_index(drop=True)\n",
        "\n",
        "    # tokenize the text and keep the result in a new column `processed_text`\n",
        "    wiki_pages[\"processed_text\"] = wiki_pages[target_column].parallel_apply(\n",
        "        partial(tokenize, stopwords=stopwords)\n",
        "    )\n",
        "    # save the result to a pickle file\n",
        "    wiki_pages.to_pickle(wiki_cache_path, protocol=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-2y8Oylk2yTu"
      },
      "outputs": [],
      "source": [
        "# Build the TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(\n",
        "    min_df=min_df,\n",
        "    max_df=max_df,\n",
        "    use_idf=use_idf,\n",
        "    sublinear_tf=sublinear_tf,\n",
        "    dtype=np.float64,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vpVWdmhP2yTu"
      },
      "outputs": [],
      "source": [
        "# Filter Wiki docs by a length\n",
        "wiki_pages = wiki_pages[\n",
        "    wiki_pages['processed_text'].str.len() > min_wiki_length\n",
        "]\n",
        "corpus = wiki_pages[\"processed_text\"].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NrN9SJc02yTu"
      },
      "outputs": [],
      "source": [
        "# Start to encode the corpus with TF-IDF\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "\n",
        "# fit_transform will do the following two steps:\n",
        "# 1. fit: learn the vocabulary and idf from the corpus\n",
        "# 2. transform: transform the corpus into a vector space\n",
        "# Note the result is a sparse matrix, which contains lots of zeros for each row."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download dataset\n",
        "# Link: https://github.com/IKMLab/CFEVER-data/blob/main/data/train.jsonl -O data/train.jsonl\n",
        "# Drag the file to the left (file system) manually"
      ],
      "metadata": {
        "id": "fJXlaqjHHLlc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LVGQMB4S2yTu"
      },
      "outputs": [],
      "source": [
        "train = load_json(\"data/train.jsonl\")[:num_of_samples]\n",
        "train_df = pd.DataFrame(train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XnOnBcdT2yTu"
      },
      "outputs": [],
      "source": [
        "# Perform the prediction for document retrieval\n",
        "# If you use Google Colab, do not use parallel_apply due to the memory limit.\n",
        "\n",
        "# train_df[\"predictions\"] = train_df[\"claim\"].parallel_apply(\n",
        "train_df[\"predictions\"] = train_df[\"claim\"].progress_apply(\n",
        "    partial(\n",
        "        get_pred_docs_sklearn,\n",
        "        tokenizing_method=partial(tokenize, stopwords=stopwords),\n",
        "        vectorizer=vectorizer,\n",
        "        tf_idf_matrix=X,\n",
        "        wiki_pages=wiki_pages,\n",
        "        topk=topk,\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3V8qnYu-2yTv"
      },
      "outputs": [],
      "source": [
        "precision = calculate_precision(train, train_df[\"predictions\"])\n",
        "recall = calculate_recall(train, train_df[\"predictions\"])\n",
        "results = {\n",
        "    \"exp_name\": exp_name,\n",
        "    \"f1_score\": 2.0 * precision * recall / (precision + recall),\n",
        "    \"precision\": precision,\n",
        "    \"recall\": recall,\n",
        "}\n",
        "\n",
        "# This helps you to adjust the hyperparameters\n",
        "save_results_to_markdown(results, output_file=\"grid_search_results.md\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mUGI41cE2yTv"
      },
      "outputs": [],
      "source": [
        "# Do not modify this cell.\n",
        "# This cell is for your scores on the training set.\n",
        "\n",
        "train = load_json(\"data/train.jsonl\")\n",
        "train_df = pd.DataFrame(train)\n",
        "\n",
        "# Perform the prediction for document retrieval\n",
        "train_df[\"predictions\"] = train_df[\"claim\"].progress_apply(\n",
        "    partial(\n",
        "        get_pred_docs_sklearn,\n",
        "        tokenizing_method=partial(tokenize, stopwords=stopwords),\n",
        "        vectorizer=vectorizer,\n",
        "        tf_idf_matrix=X,\n",
        "        wiki_pages=wiki_pages,\n",
        "        topk=topk,\n",
        "    )\n",
        ")\n",
        "precision = calculate_precision(train, train_df[\"predictions\"])\n",
        "recall = calculate_recall(train, train_df[\"predictions\"])"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.9.16 ('hw3')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16 (main, Mar  8 2023, 14:00:05) \n[GCC 11.2.0]"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "7cbde34494a11b1624bcc1eb71dba5ceb4f0ff8d7a6c820b0d6fa32591a5e209"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}